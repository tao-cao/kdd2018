{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TC- Notebook 3 - Movie Critics.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/tao-cao/kdd2018/blob/master/TC_Notebook_3_Movie_Critics.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "gYFEQZku4Wya",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9b4f8fd9-7085-4bfd-b80a-7e84ec2dd6c6"
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "keras.__version__"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.1.6'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "metadata": {
        "id": "Wv6WVal-4Wyc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Notebook 3"
      ]
    },
    {
      "metadata": {
        "id": "VaN78A9Z4Wyd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Using WordEmbeddings and RNNs to classify IMDB reviews: a binary classification problem\n",
        "\n",
        "This notebook contains code samples that have been adapted from Francois Chollet's Deep Learning With Python.\n",
        "\n",
        "----\n",
        "In this notebook we are going to introduce how to use Embeddings and [Recurrent Neural Network(RNN)](https://keras.io/layers/recurrent/) layers in Keras. We are going to introduce the following layers:\n",
        "* `Embedding`\n",
        "* `SimpleRNN`\n",
        "* `LSTM`\n",
        "\n",
        "We are going look at how to train networks with limited input text data using pre-trained embeddings. Tokenizing and preprocessing input text data will also be illustrated end to end. "
      ]
    },
    {
      "metadata": {
        "id": "ncWBG8qQ4Wye",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Problem \n",
        "In this notebook, we will learn to classify movie reviews into \"positive\" reviews and \"negative\" reviews, just based on the text content of the reviews."
      ]
    },
    {
      "metadata": {
        "id": "kTbvTQhl4Wyf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# First, a note on using word embeddings\n",
        "\n",
        "Another popular and powerful way to associate a vector with a word is the use of dense \"word vectors\", also called \"word embeddings\". \n",
        "While the vectors obtained through one-hot encoding are binary, sparse (mostly made of zeros) and very high-dimensional (same dimensionality as the \n",
        "number of words in the vocabulary), \"word embeddings\" are low-dimensional floating point vectors \n",
        "(i.e. \"dense\" vectors, as opposed to sparse vectors). \n",
        "Unlike word vectors obtained via one-hot encoding, word embeddings are learned from data. \n",
        "It is common to see word embeddings that are 256-dimensional, 512-dimensional, or 1024-dimensional when dealing with very large vocabularies. \n",
        "On the other hand, one-hot encoding words generally leads to vectors that are 20,000-dimensional or higher (capturing a vocabulary of 20,000 \n",
        "token in this case). So, word embeddings pack more information into far fewer dimensions. "
      ]
    },
    {
      "metadata": {
        "id": "dzLO9JWc4Wyf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![word embeddings vs. one hot encoding](https://s3.amazonaws.com/book.keras.io/img/ch6/word_embeddings.png)"
      ]
    },
    {
      "metadata": {
        "id": "P8xWAY7x4Wyg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "There are two ways to obtain word embeddings:\n",
        "\n",
        "* Learn word embeddings jointly with the main task you care about (e.g. document classification or sentiment prediction). \n",
        "In this setup, you would start with random word vectors, then learn your word vectors in the same way that you learn the weights of a neural network.\n",
        "* Load into your model word embeddings that were pre-computed using a different machine learning task than the one you are trying to solve. \n",
        "These are called \"pre-trained word embeddings\". \n",
        "\n",
        "Let's take a look at both."
      ]
    },
    {
      "metadata": {
        "id": "lk8MQCPd4Wyg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Embedding\n",
        "\n",
        "# The Embedding layer takes at least two arguments:\n",
        "# the number of possible tokens, here 1000 (1 + maximum word index),\n",
        "# and the dimensionality of the embeddings, here 64.\n",
        "embedding_layer = Embedding(1000, 64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rRS9PsQu8IYk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c6b3dc05-a3be-4b43-eca9-de4334b1f008"
      },
      "cell_type": "code",
      "source": [
        "embedding_layer"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.layers.embeddings.Embedding at 0x7f30d73b1250>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "tjULAx154Wyj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "The `Embedding` layer is best understood as a dictionary mapping integer indices (which stand for specific words) to dense vectors. It takes \n",
        "as input integers, it looks up these integers into an internal dictionary, and it returns the associated vectors. It's effectively a dictionary lookup."
      ]
    },
    {
      "metadata": {
        "id": "IIofYyjf4Wyk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "The `Embedding` layer takes as input a 2D tensor of integers, of shape `(samples, sequence_length)`, where each entry is a sequence of \n",
        "integers. It can embed sequences of variable lengths, so for instance we could feed into our embedding layer above batches that could have \n",
        "shapes `(32, 10)` (batch of 32 sequences of length 10) or `(64, 15)` (batch of 64 sequences of length 15). All sequences in a batch must \n",
        "have the same length, though (since we need to pack them into a single tensor), so sequences that are shorter than others should be padded \n",
        "with zeros, and sequences that are longer should be truncated.\n",
        "\n",
        "This layer returns a 3D floating point tensor, of shape `(samples, sequence_length, embedding_dimensionality)`. Such a 3D tensor can then \n",
        "be processed by a RNN layer or a 1D convolution layer.\n",
        "\n",
        "When you instantiate an `Embedding` layer, its weights (its internal dictionary of token vectors) are initially random, just like with any \n",
        "other layer. During training, these word vectors will be gradually adjusted via backpropagation, structuring the space into something that the \n",
        "downstream model can exploit. Once fully trained, your embedding space will show a lot of structure -- a kind of structure specialized for \n",
        "the specific problem you were training your model for."
      ]
    },
    {
      "metadata": {
        "id": "q8eKDdRO4Wyk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "Let's consider IMDB movie review sentiment prediction task that you are already familiar with. Let's quickly prepare \n",
        "the data. We will restrict the movie reviews to the top 10,000 most common words, \n",
        "and cut the reviews after only 20 words. Our network will simply learn 8-dimensional embeddings for each of the 10,000 words, turn the \n",
        "input integer sequences (2D integer tensor) into embedded sequences (3D float tensor), flatten the tensor to 2D, and train a single `Dense` layer on top for classification."
      ]
    },
    {
      "metadata": {
        "id": "ptdLzLjF4Wyk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.datasets import imdb\n",
        "from keras import preprocessing"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8GRkZH1N_EsU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Number of words to consider as features\n",
        "max_features = 10000\n",
        "# Cut texts after this number of words \n",
        "# (among top max_features most common words)\n",
        "maxlen = 20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7YZo_NRx_ICC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "d4e97b48-1f51-4721-e093-6292d7991f7e"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the data as lists of integers.\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 1s 0us/step\n",
            "17473536/17464789 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SRwXJcHg_WFf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c690f179-8ce2-407d-ea4e-1a59d162cb2a"
      },
      "cell_type": "code",
      "source": [
        "(x_train.shape, y_train.shape), (x_test.shape, y_test.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(((25000,), (25000,)), ((25000,), (25000,)))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "RMhs80YX_nbu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_train_len_20 = [len(x_train[i]) for i in range(20)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mHSKj_NtBxPS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "5c76b626-1aba-4d12-e7b9-4a967a19e40f"
      },
      "cell_type": "code",
      "source": [
        "x_train_len_20"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[218,\n",
              " 189,\n",
              " 141,\n",
              " 550,\n",
              " 147,\n",
              " 43,\n",
              " 123,\n",
              " 562,\n",
              " 233,\n",
              " 130,\n",
              " 450,\n",
              " 99,\n",
              " 117,\n",
              " 238,\n",
              " 109,\n",
              " 129,\n",
              " 163,\n",
              " 752,\n",
              " 212,\n",
              " 177]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "F0Cp-bLrD2Kj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# This turns our lists of integers\n",
        "# into a 2D integer tensor of shape `(samples, maxlen)`\n",
        "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)"
      ]
    },
    {
      "metadata": {
        "id": "QUtfeuXD4Wyn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# This turns our lists of integers\n",
        "# into a 2D integer tensor of shape `(samples, maxlen)`\n",
        "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R5k7W_IoCpV4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_train_len_20_pad = [len(x_train[i]) for i in range(20)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7F_eiKxuCu84",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "629f2f2c-5236-4702-8e8a-4f57352ad13c"
      },
      "cell_type": "code",
      "source": [
        "x_train_len_20_pad"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[20,\n",
              " 20,\n",
              " 20,\n",
              " 20,\n",
              " 20,\n",
              " 20,\n",
              " 20,\n",
              " 20,\n",
              " 20,\n",
              " 20,\n",
              " 20,\n",
              " 20,\n",
              " 20,\n",
              " 20,\n",
              " 20,\n",
              " 20,\n",
              " 20,\n",
              " 20,\n",
              " 20,\n",
              " 20]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "0qcbGQKO4Wyp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Exercise 1: Build and train a Sequential model with the following specs:\n",
        "* Embedding Layer with maximum number of tokes to be 10000 and embedding dimensionality as 8. Let the input_length be the maximum length of each review i.e 20 as seen previously.\n",
        "* Flatten the 3D embedding output to 2D.\n",
        "* Dense Layer which is the classifier.\n",
        "* Compile the model with a 'rmsprop' optimizer. Can you guess what loss we need to use?\n",
        "* Let accuracy be one of the metrics we are interested in.\n",
        "* Run the model on the above training data. "
      ]
    },
    {
      "metadata": {
        "id": "AYgSXXTn4Wyp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "outputId": "316015af-aa19-4b10-fe98-77b805b597ab"
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten, Dense\n",
        "\n",
        "model = Sequential()\n",
        "# Add an Embedding Layer with maximum number of tokes to be 10000 and embedding dimensionality as 8. \n",
        "# Let the input_length be the maximum length of each review i.e 20 as seen previously.\n",
        "# After the Embedding layer, \n",
        "# our activations have shape `(samples, maxlen, 8)`.\n",
        "\n",
        "# We flatten the 3D tensor of embeddings \n",
        "# into a 2D tensor of shape `(samples, maxlen * 8)`\n",
        "# ...\n",
        "# We add a Dense classifier on top\n",
        "# ...\n",
        "\n",
        "# Exercise 1: Solution\n",
        "model.add(Embedding(10000, 8, input_length=maxlen))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=32,\n",
        "                    validation_split=0.2)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 20, 8)             80000     \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 160)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 161       \n",
            "=================================================================\n",
            "Total params: 80,161\n",
            "Trainable params: 80,161\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/10\n",
            "20000/20000 [==============================] - 4s 210us/step - loss: 0.6759 - acc: 0.6050 - val_loss: 0.6398 - val_acc: 0.6814\n",
            "Epoch 2/10\n",
            "20000/20000 [==============================] - 3s 169us/step - loss: 0.5657 - acc: 0.7427 - val_loss: 0.5467 - val_acc: 0.7206\n",
            "Epoch 3/10\n",
            "20000/20000 [==============================] - 3s 169us/step - loss: 0.4752 - acc: 0.7808 - val_loss: 0.5113 - val_acc: 0.7384\n",
            "Epoch 4/10\n",
            "20000/20000 [==============================] - 3s 170us/step - loss: 0.4263 - acc: 0.8077 - val_loss: 0.5008 - val_acc: 0.7452\n",
            "Epoch 5/10\n",
            "20000/20000 [==============================] - 3s 169us/step - loss: 0.3930 - acc: 0.8258 - val_loss: 0.4981 - val_acc: 0.7538\n",
            "Epoch 6/10\n",
            "20000/20000 [==============================] - 3s 169us/step - loss: 0.3668 - acc: 0.8395 - val_loss: 0.5014 - val_acc: 0.7530\n",
            "Epoch 7/10\n",
            "20000/20000 [==============================] - 3s 168us/step - loss: 0.3435 - acc: 0.8533 - val_loss: 0.5052 - val_acc: 0.7520\n",
            "Epoch 8/10\n",
            "20000/20000 [==============================] - 4s 218us/step - loss: 0.3223 - acc: 0.8657 - val_loss: 0.5132 - val_acc: 0.7486\n",
            "Epoch 9/10\n",
            "20000/20000 [==============================] - 3s 167us/step - loss: 0.3022 - acc: 0.8766 - val_loss: 0.5213 - val_acc: 0.7490\n",
            "Epoch 10/10\n",
            "20000/20000 [==============================] - 3s 170us/step - loss: 0.2839 - acc: 0.8860 - val_loss: 0.5303 - val_acc: 0.7466\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mk_YCXqg4Wys",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If you used a batch size of 32 and ran the model for 10 epochs you should get to a validation accuracy of ~76%, which is pretty good considering that we only look at the first 20 words in every review. But \n",
        "note that merely flattening the embedded sequences and training a single `Dense` layer on top leads to a model that treats each word in the \n",
        "input sequence separately, without considering inter-word relationships and structure sentence (e.g. it would likely treat both _\"this movie \n",
        "is shit\"_ and _\"this movie is the shit\"_ as being negative \"reviews\"). It would be much better to add recurrent layers or 1D convolutional \n",
        "layers on top of the embedded sequences to learn features that take into account each sequence as a whole."
      ]
    },
    {
      "metadata": {
        "id": "-TsEvwcvFkWd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GmsQTiOoFdgR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_fun(loss,val_loss):\n",
        "\n",
        "  epochs = range(1, len(loss) + 1)\n",
        "  plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "\n",
        "  plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "  plt.title('Training and validation loss')\n",
        "\n",
        "  plt.xlabel('Epochs')\n",
        "\n",
        "  plt.ylabel('Loss')\n",
        "\n",
        "  plt.legend()\n",
        "\n",
        "  plt.show()\n",
        "  \n",
        "def plot_acc(loss,acc,val_acc):\n",
        "  \n",
        "  epochs = range(1, len(loss) + 1)\n",
        "\n",
        "  plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "\n",
        "  plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "\n",
        "  plt.title('Training and validation accuracy')\n",
        "\n",
        "  plt.xlabel('Epochs')\n",
        "\n",
        "  plt.ylabel('Loss')\n",
        "\n",
        "  plt.legend()\n",
        "\n",
        "\n",
        "  plt.show()\n",
        "  \n",
        "\n",
        "def plt_all(history):\n",
        "\n",
        "  loss = history.history['loss']\n",
        "\n",
        "  val_loss = history.history['val_loss']\n",
        "  \n",
        "  plot_fun(loss,val_loss)\n",
        "\n",
        "\n",
        "  acc = history.history['acc']\n",
        "\n",
        "  val_acc = history.history['val_acc']\n",
        "\n",
        "  plot_acc(loss,acc,val_acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Bxnfp3aHFtmk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        },
        "outputId": "9b72bd43-d981-4b8e-a51b-854654248bc8"
      },
      "cell_type": "code",
      "source": [
        "plt_all(history)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEVCAYAAAAPRfkLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nREQk6f8HI02LaVn1K+4AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f30d5e22450>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEVCAYAAAAPRfkLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFLcAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f311dfe92d0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "9sioZZsy4Wys",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## What if we increase the number of words that we consider in each IMDB review? Let maxlen now be 200 instead of 20."
      ]
    },
    {
      "metadata": {
        "id": "K3yW_lmy4Wyt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "outputId": "015be1b7-caa6-48c5-8e25-0d9d3c015a28"
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten, Dense\n",
        "# Number of words to consider as features\n",
        "max_features = 10000\n",
        "# Cut texts after this number of words \n",
        "# (among top max_features most common words)\n",
        "maxlen = 200\n",
        "\n",
        "# Load the data as lists of integers.\n",
        "#(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "\n",
        "# This turns our lists of integers\n",
        "# into a 2D integer tensor of shape `(samples, maxlen)`\n",
        "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(10000, 8, input_length=maxlen))\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "model.summary()\n",
        "\n",
        "history_200 = model.fit(x_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=32,\n",
        "                    validation_split=0.2)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 200, 8)            80000     \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 1601      \n",
            "=================================================================\n",
            "Total params: 81,601\n",
            "Trainable params: 81,601\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/10\n",
            "20000/20000 [==============================] - 3s 170us/step - loss: 0.6630 - acc: 0.6263 - val_loss: 0.5988 - val_acc: 0.7032\n",
            "Epoch 2/10\n",
            "20000/20000 [==============================] - 3s 168us/step - loss: 0.5257 - acc: 0.7530 - val_loss: 0.5161 - val_acc: 0.7340\n",
            "Epoch 3/10\n",
            "20000/20000 [==============================] - 3s 172us/step - loss: 0.4584 - acc: 0.7835 - val_loss: 0.4972 - val_acc: 0.7460\n",
            "Epoch 4/10\n",
            "20000/20000 [==============================] - 3s 172us/step - loss: 0.4285 - acc: 0.8012 - val_loss: 0.4917 - val_acc: 0.7516\n",
            "Epoch 5/10\n",
            "20000/20000 [==============================] - 3s 172us/step - loss: 0.4090 - acc: 0.8120 - val_loss: 0.4931 - val_acc: 0.7556\n",
            "Epoch 6/10\n",
            "20000/20000 [==============================] - 3s 170us/step - loss: 0.3939 - acc: 0.8213 - val_loss: 0.4950 - val_acc: 0.7582\n",
            "Epoch 7/10\n",
            "20000/20000 [==============================] - 3s 171us/step - loss: 0.3809 - acc: 0.8284 - val_loss: 0.4983 - val_acc: 0.7554\n",
            "Epoch 8/10\n",
            "20000/20000 [==============================] - 3s 171us/step - loss: 0.3690 - acc: 0.8354 - val_loss: 0.5012 - val_acc: 0.7560\n",
            "Epoch 9/10\n",
            "20000/20000 [==============================] - 3s 171us/step - loss: 0.3567 - acc: 0.8445 - val_loss: 0.5040 - val_acc: 0.7572\n",
            "Epoch 10/10\n",
            "20000/20000 [==============================] - 3s 169us/step - loss: 0.3447 - acc: 0.8498 - val_loss: 0.5103 - val_acc: 0.7566\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EdTgi0JCHM8Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        },
        "outputId": "8e6e8edc-4827-4bbf-a3ae-7ce00a5cba23"
      },
      "cell_type": "code",
      "source": [
        "plt_all(history_200)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEVCAYAAAAPRfkLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEZE4BQUREYlTUBARkTgFBRERifv/t4c5RLK/ucUAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f30d60f0510>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEVCAYAAAAPRfkLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzt3XmcFNW9//9XzwwDDCIMMGwGFxA+\nghoTXHEDg7hEjRd/evWnCRI1qBGFGGMwVw1GiRo3NCaKC6JRUW8Ul0hUXPASUa/iciPLBwXFBYRR\nWQYYlpnp7x9VXdMzdMPATE839Pv5eMyjq07XcurMTH3qnFN1KhaPxxEREQEoyHYGREQkdygoiIhI\nREFBREQiCgoiIhJRUBARkYiCgoiIRIqynQHJPjO7CzgqnO0FLAYqw/kD3b1iK7Y1Dxjo7ks3s8z1\nwCJ3v3sbs9zkzOxl4GF3n9QE24oDPYADgZPc/Zxt3Z+Z/cLd7w2nt1i2Io2loCC4+4WJaTP7DPip\nu/9rG7e1VwOWuWJbtr29cfcpwJRtXd/MugKXA/eG29ti2Yo0loKCbJGZTQfeAE4BzgUWAA8CuwMt\ngT+7+63hsomr5D2B64HpwH8ArYDh7v66mU0CPnH368IgdH243R7Ao+7+63BbvwNGA4uAB4DL3X33\nFPk7D/g1wd/zEuBn7r7IzIYDJwCrgCOAKuA0d59tZj2ByUAn4C1S/C+Y2Y+BG91936S0D4AxwPvp\nyiBp2eEEAfboze3PzH4CjAOKgdXAue7+ATAT+F5YQ/g+sB7o4e5fmtklwAUETcAOnOfu5WHZLgIO\nBfoA84GT3X1tvbyVhGX6g3C/T7r7ZeF3PYFJQHdgOXC+u7+3mfTPSLqQSMwDX4bH8DjQ390HbuZY\nMbPfAueHv6d/AL8BvgJOdPd3w2VGAke7+3/U/31J01CfgjTU/sDe7j4TuBL4NLxyHQxcb2Y9Uqzz\nQ+Atd+8L/DVcL5UjgQHhPi42s++Z2d4EV8n7EZzQ/zPVimbWGbgTGOLuvYFPgKuSFvkx8Fd37wO8\nRhBkAG4AXnH3XsDtwGEpNv8ywUl5j3BfewDfC9MbWgYJKfdnZkUEweUX7m7AM8DN4TrnAJ+7+17u\nviHpmA8hOGEOCvf/OUFgTTgNOJ2gKbAMGJoiPxcCbYG9gP7AcDM7PPzuHmCyu+9JcAL/2xbSN6cT\n8EEYENIea7jv8wh+3/sAhxNchDwBnJm0vaHAYw3Yr2wjBQVpqKnuXhNOXwJcDODuC4GvgT1SrFPh\n7s+E0+8Bu6bZ9qPuXu3ui4GlBDWGI4Hp7r7E3dcBE1Ot6O7LgJ3d/cswaQbQM2mROe4+K0UejiS4\ngsXd/xeYl2LbG4DngJ+ESUOBp929aivKICHl/sJtdXb3t9LkP5UTgL+Hxw5wH3BM0vfPu/t34bb/\nTYpyd/dbCGoQcXdfDswGeppZK4L+pcnhos8AB6dL30I+AVoQNqFt4Vh/HOa7Iiz3QcBT4f5ON7MC\nM+sAHEDwO5EMUfORNNR3SdMHElwZ7wpUA91IfYGxMmm6GihMs+1Uy5XW2+dXqVY0s0LgD2GzRCHB\n1e/8BuShQ73vlqfJ29+BUQRX9/8BXBumN7QMEja3v0vM7GyCZqhWwJYGJCsjuBkgeVudk+a3WO5m\n1hu41cz2CpfpQdCc1CE8jpUA7h4HVptZ91TpW8gnQLW7r0qaT3esnZKPKam5600z2wAMDPP4oruv\nacB+ZRuppiDb4mGCk2WfsPmiPAP7WAXslDTfLc1ypxNcyR8ZNkn8voHbXw60S5ovS7Pci8APwpNo\nH+DVMH1ryyDl/szsUOC3wE/C/J/XgLwvBTomzXcM07bGX4CPgL3C/H8Qpn9LcKLuGOYvZmZ7pks3\nsxibBp7SVDvcwrF+QxAYEst2NLPEMT5G0CR2KmFtSzJHQUG2RWdglrvHw6u+NtQ9gTeF/wWOMrNO\nZtYSOHszefnM3b8JTyL/2cC8vEnY1h6erPZMtZC7rycIDH8CnnH36qT9bk0ZpNtfZ2AZ8HnY+Xs2\n0CY82W4Edgrb4pM9D5ySdNI8P0zbGp2B99292syGAL2BncLjfQkYHi53LEHTYbr0OEHn/n7hsZ1O\nUANIt890x/os8BMzKw2P9+lwHwCPEpTdocDUrTxO2UoKCrItrgKmmNn/EZwIJwD3mlmvptpB2O7+\nIMFdPq8StCOnalaZDHQ0s0/C6SuBHmZ2yxZ2cTlwkpktAEYC0zaz7N8Jmo6eSErb2jJIt78XCJpN\nFhCcdMcTNNH8Hfg/gia0r8NmKiAqmxuAGeGdSe2B/9rC8dZ3HXCLmX1E0DRzDXCNmR1GcAV/kpkt\nDJdLdPSmS78WuDTcVl9gTpp9pj3WsJ/hJoIayxyC/p/J4fH+m6Cm8qK7V6bYrjShmN6nILnKzGLh\nlShmdgJwnbv/MMvZkiwws6nAne6umkKGqaNZcpKZlQHzzKw/wS2X/0nQBCN5Jqy97E5Q05AMU/OR\n5CR3LydoEnmF4G6iDsDYbOZJmp+ZTSS4HXl40i3RkkFqPhIRkYhqCiIiEtnu+xTKyyu2+6pOaWkJ\ny5ev3fKCeUBlUZfKoy6VR63GlkVZWdtYqnTVFHJAUVG6B33zj8qiLpVHXSqPWpkqCwUFERGJKCiI\niEhEQUFERCIKCiIiElFQEBGRSEZvSTWz24BDCAYyG+Xu7yR9dxHBK/uqgXfdfXT4+sJrCQbMApjm\n7uMymUcRke3JlClFjB9fzPz50KdPCaNHb2Do0Kom237GgoKZDQR6u/sAM+tL8Kj6gPC7nQleJ7in\nu1eZ2UvhKwYBHk+8K1ZERGpNmVLE+ee3jubnzi0M5yubLDBksvloMMGY6Lj7XKA0DAYAG8KfxFjx\nJdR9y5aIiNQzfnxxyvTbb0+dvi0y2XzUFZiVNF8epq1y93Vmdg2wEKgEHnP3+eHLRwaa2QsE73a9\nzN3f39xOSktLcu6BlhtuuIHZs2dTXl5OZWUlu+66K+3atePOO+9Mu05ZWVsAnnrqKdq2bcuQIUNS\nLjdu3DiGDRtGjx6be0f89i1RFhJQedSVz+Uxf3669MImK5fmHOYieqQ6rDH8juD1hquAV81sP+At\noNzdnzezAcBDwL6b22hTPPJe20ZXQJ8+NY1uozv33IsAmDr1ORYuXMDIkaMBKC+vSLl8WVnb6Lsj\njhiy2WVHjLhks99v75LLQlQe9eV7efTpU8LcuZteBPfpU015+dadC9MFkUwGhcUENYOE7gSv7YPg\n7UwL3f0bADObAezv7hOBeQDu/qaZlZlZYdIrEJtcc7TRJbz33rs89tjDrF27lpEjf8X7789i+vRX\nKCyMccABh3DOOSO4//4JtG/fnj326MVTTz1BLFbAokWfMmjQYM45ZwQjR47g0ksv57XXXmHNmtV8\n/vkivvrqSy655NcMGHAYDz88iZdffonu3XehqqqKM844i/79D4jy8M47b3PffXfTokUL2rZtyx/+\ncAMtWrRg/PibmTPnIwoLC/nNb66gZ889U6aJ5LOmvoDcWqNHb6hzvkoYNWpDk+0jk0HhJYJX/E0I\nX5Sy2N0TIf4zoK+ZtQ5fr3cAMNXMLge+cPfJZrYPQa0hYwEBNt9Gl4lf9oIFnzB58lMUFxfz/vuz\n+Otf76NLl3YcddSPOP30M+ssO2fObB599Elqamo47bSTOOecEXW+X7ZsKTfffAdvvTWTZ555kr33\n3oennvpvJk9+kjVr1nDGGadwxhln1VmnoqKC3//+Orp334Vrr72at99+k5YtW7Js2VLuuWcSH3zw\nHq+8Mo1vv/12kzQFBclnzXkBmU6wn0puv72Y+fML6dOnmlGjtpO7j9x9ppnNMrOZQA1wUXjL6Up3\nn2JmNwGvmVkVMNPdZ5jZp8DfzOyCMG/nZip/CfPnp+5rT5feWHvu2Zvi4iAQtWrVipEjR9C6dUtW\nrFjBqlWr6ixrthetWqV7Bzp8//s/AKBz586sXr2aL7/8gp49e9GyZStatmxF3757b7JO+/btufHG\n66iurmbx4q/Yf/8DWb78O/bddz8AfvCD/vzgB/155JEHN0kTyWfNfQGZztChVQwdWhU2pTX9iLEZ\n7VNw9zH1kj5M+m4CwcvOk5f/Ejgqk3mqr0+fmjRtdJl5yVOLFi0A+PrrJTz++CNMnPgIu+3WheOO\nO36TZQsLN9+Bnvx9PB4nHoeCgtpgFksxMO7111/LTTeNZ/fd9+DWW28EoKCgkHi87vGmShPJZ819\nAZktO9bRbIPRo1O3xTVlG10qK1asoLS0lJKSEmbPns3XX3/Nxo0bG7XNbt26sXDhAqqqqli+fDnz\n5s3dZJk1a1bTpUtXKioqeO+9WWzcuJG+ffvx3nvvAjB//jxuueXGlGki+SzdhWKmLiCzZbt/yU5j\n1W2jCzqPmrqNLpXevfvQunUJF154DgcffBAnn3wKt9xyI9///n7bvM0OHToyZMhx/OIXw9httz3o\n12/vTWobp5xyGhdeeC49euzKWWcNY+LEe7jrronsttse/PKX5wHw61+PoVevPZkx4/U6aSL5rDk6\neXPBdv+O5h3hzWtNeZvd1KnPMWTIcRQWFjJs2Bnceuuf6dy5S5Nsuznk+y2H9ak86sp2eUyZUtTs\nF5DpNLYs0r15Le9rCjuab7/9lhEjzqZFi2KOOea47SogiOS6RCfvjkw1hRyQ7aufXKKyqEvlEah9\nPiC4DbO5nw/IRaopiEheyoXnA/JJ3t99JCK5rTkGgZNaCgoiktPy5fmAXKFSFZGcli/PB+QKBYUM\nOP/8n2/y4Njdd9/J5MkPp1z+7bff5sorLwdgzJhLN/n+yScf5/77J2ySnvDJJx/z+eeLAPj9769g\n/fp125p1kZyTrQdM85WCQgYMGXIsr746rU7a9OmvcvTRx2xx3RtuuHWr9/f666/yxRefA3DNNdfT\nsmX68ZJEtsaUKUUMHFhCt247MXBgCVOmNP+9KUOHVjFhQiX9+lVTVAT9+lUzYYI6mTNFdx9lwODB\nx3Dhhefyy18G7z6YN28uZWVllJV1Tjl0dbITThjM88+/wrvv/i933HELHTp0pGPHTtFQ2OPGjaW8\nfBmVlZWcc84IunbtxjPPPMXrr79KaWkpV199BQ899DirV1dw/fV/YOPGjRQUFDBmzFXEYjHGjRtL\n9+678MknH9OnjzFmzFV19v/SS//k739/nMLCAnbfvRe//e1/UVVVxXXX/Z6lS5dQXNySK6+8htLS\nDpuklZV1brYylszLpbt+Mj0InNTa4YPC2LEtee65pj3Mk06qYuzY9Wm/Ly3tQPfuuzBnzkf067cP\nr746jSFDjgNSD13dvXunTbYxYcKdXHXVtfTu3YfLLruE7t13oaJiFQcddAjHH38iX331JVddNYaJ\nEx/m4IMHMGjQYPr12yda/7777ubEE09m8OBjeO21l5k48R7OPfd83OdyzTV/pLS0A0OH/piKigra\ntq192UZlZSW33PJn2rZty0UX/YIFCz5hzpyP6NixI2PHjuPll1/kX//6H4qKijZJGzr01CYsZcm2\nXBkVVJrXDh8UsmXIkON45ZVp9Ou3D2+88T/cdddEIPXQ1amCwpIlS+jduw8QDF29fv162rbdmblz\nZ/Pss08RixWwatXKtPt3n8sFF4wEoH//A5g06T4AdtmlBx07Bvvr1KmMNWtW1wkKO++8M1dc8WsA\nFi36lJUrV+A+jwMOOBCAo48+FoCbb75hkzTZseiun/y0wweFsWPXb/aqPlMGDjyKhx6ayJAhx9Kj\nx67svPPOQOqhq1NJHgI78dT5tGkvsGrVKv7yl/tYtWoV5533s83kIBatt3FjFbFYsL36A+QlP9G+\nceNGbr31T0ya9CgdO3bi8stHh+sUUFNT98HxVGmyY2nuYeUlNyjkZ0hJSRt69erNQw89EDUdQeqh\nq1Pp1KmMzz//jHg8zvvvzwKC4ba7detOQUEBr7/+arRuLBajurruC+qSh77+4INZ7LVX3y3mee3a\nNRQWFtKxYyeWLv2aefPmUlVVxV579eO9994B4I03ZvDQQxNTpsmORXf95CcFhQwaMuQ43nnnbQ4/\n/MgoLTF09Z/+NI6zzhrGww9Pory8fJN1R4z4JVde+Vt++9tfRYPaDRr0I2bOnMGoURfSunVrOnfu\nzAMP3Mt++/2Q8eNv4t13/zda/7zzLuCFF6ZyySUXMHXqPzj33PO3mN927dpz4IEHc955w3jggXs5\n88yfcccdtzJ48DFUVlYycuQInnhiMscffyJHH33sJmnSdBJ3/RQVkSN3/cR110+e0IB4OUCDntVS\nWWx610+CTsj6+0iWqQHxVFMQyTEa60eySUFBJMforh/JJv2VieQYjfUj2aSgIJJjdNePZJOCgkiO\n0Vg/kk07/MNrItsjjfUj2aKagoiIRBQURJLkwlDRItmkv3iRUC4NFS2SLaopiIT00JiIgoJIRA+N\niSgoiET00JiIgoJIRA+NiSgoiEQ0VLSI7j4SqSPx0JhIvlJNQUREIgoKkhNy4U1jIpLh5iMzuw04\nBIgDo9z9naTvLgJ+ClQD77r7aDNrAUwCdgvTf+7uCzOZR8k+PTQmkjsyVlMws4FAb3cfAJwL3JH0\n3c7Ab4Aj3P1woJ+ZHQKcCawI08YB12cqf5I79NCYSO7IZPPRYOBpAHefC5SGwQBgQ/izk5kVASXA\nd+E6U8JlXgYOy2D+JEfooTGR3JHJ5qOuwKyk+fIwbZW7rzOza4CFQCXwmLvPN7Ou4XK4e42Zxc2s\n2N3T3iheWlpCUVFh5o6imZSVtc12FrKmXz/4979TpcfyulwSVAZ1qTxqZaIsmrM3L5aYCGsMvwP6\nAKuAV81sv82tk87y5dv/WPPBmPkV2c5G1owcWbdPIeGiiyopL8/vPoV8/9uoT+VRq7FlkS6gZLJ+\nvpigZpDQHVgSTvcFFrr7N2EtYAawf/I6YadzbHO1BNkx6E1jIrkjk0HhJeBUADPrDyx290RY+wzo\na2aJy8MDgI/DdU4L004CXstg/iSHDB1axfTpa9m4EaZPX6uAIJIlGWs+cveZZjbLzGYCNcBFZjYc\nWOnuU8zsJuA1M6sCZrr7DDMrBIaY2b+A9cDwTOVPREQ2FYvH49nOQ6OUl1ds3weA2kmTqSzqUnnU\npfKo1QR9Cin7bHXPn4iIRBQUREQkoqAgIiIRBQUREYkoKIiISERBQUREIgoKEr3LoFu3nfQuA5E8\np//+PKd3GYhIMtUU8pzeZSAiyRQU8pzeZSAiyfSfn+f69KnZqnQR2bEpKOS50aNTj0w+apRGLBfJ\nRwoKea7uuwziepeBSJ7T3UfC0KFVCgIiAqimICIiSRQUREQkoqAgIiIRBQUREYkoKIiISERBQURE\nIgoKIiISUVAQEZGIgoKIiEQUFEREJKKgICIiEQUFERGJKCiIiEhEQUFERCIKCiIiElFQEBGRiF6y\nI5IlVVWwcmWMlSth+fIYK1fGWL48xooVtdMtW0LLlsWUlsajnw4d4rRvH3y2bQuxWLaPRHYkCgoi\njRCPQ0UFrFgRi36ST+4rVhDNr1wZq7NcRUVDz+Yt035TVBQEiOSAUVpKnSBSP5CUlsZp1appjj+X\nVVXBunWwfn2M9evrTwefwU8s+m7duvTLJaY3bIhRUACFhXGKiqCoCAoLCafj0XSqtNr0OC1abHmZ\nutupu8/evcnI71FBQSRUVQXLlsVYsiTGsmUFrFiR+mRf+xmc8KurG36p3qZNcHLu0aOG0tI47drF\nw09o3z6+yU+nTm1YuHAtK1bE+O67YL/ffRfkJzGd+FywoICamoblpaQk2G9yoNhcICkuDsqnujpG\nVVVimnA6Fk2nSqtNj9VbZsvbS15348agVrRqVevwxL35k/fW/F62VzNmFGBW06TbVFDIoilTihg/\nvpj586FPnxJGj96gdyVnQDwOq1bBkiUFLFkS4+uvYyxZUsDXX9dOL1kSo7w8Rjy+5RNJcXHihB2n\nV694vZN77WftyT044bdrF5xct0ZZGXzve9UNWramJjjOVEEj+Sf5+0WLCpg9e3s7eQanrVgsTuvW\nhE1sQe2nXbs4LVsmfoIr6drpeLhs8nQ8XCZ5uu5yibTEZ3Fx8De1rcEvsd7WBsj6aV27FrPbbk0b\nEGpLV5rdlClFnH9+62h+7tzCcL5SgWErbNgAS5fGwpN9Qb3P4IS/dGmMtWvTn/hatozTtWucgw+u\nplu3OF26xOnSpYYOHVJfwbdunZvt+AUFRAEI4g1eb8MGoiBRGzSoE1A2boxtUxPH1jSPNGR73bvv\nxOrVFbRsGcxn//eQqpwbXvaNUVZWTHl50283o0HBzG4DDiEopVHu/k6YvgvwSNKiPYExQDFwLbAg\nTJ/m7uMymcdsGT8+9SXj7bcXKygQXIktX050RV/7Wfek/8036W+gi8USV/M1dOsWp2vXxGecbt1q\nos/27XPh5JI9xcXQuXOczp2b52TWGGVl2c7Bji9jQcHMBgK93X2AmfUFJgIDANz9K2BQuFwRMB14\nFjgVeNzdL8tUvnLF/PmpT2bp0ndEa9fCp58WsHBh7c+XX8KiRW1YujTG+vXpz9QlJXG6dYtjVhWd\n3BMn/MTJv0uXoDNPRBoukzWFwcDTAO4+18xKzWxnd19Vb7nhwJPuvtrMMpid3NKnTw1z5xamTN+R\nbNgAixYVsHBh0BGaHAAWL940ABYUQOfO0K9fDV27Jq7mN73K162YIpmRyaDQFZiVNF8eptUPCucB\nxyTNDzSzF4AWwGXu/n4G85g1o0dvqNOnkDBq1IYs5KZxqqvhyy+Dk/6nnxZEJ/8FCwr44otYyjti\ndtmlhiOOqKJnzxp69aqJPvfffydWrFiThaMQEWjejuZNzgxmNgCYl1R7eAsod/fnw+8eAvbd3EZL\nS0soKtr0ijvXjRgBO+8M118Pc+ZAv35wxRVwxhmbBopcEI/D4sUwfz58/HHt58cfw4IFQY2gvi5d\n4NBDoU+f4J7qxGevXlBSUkC6B+rLytpm9mC2MyqPulQetTJRFpkMCosJagYJ3YEl9ZY5EXg5MePu\n84B54fSbZlZmZoXunvaevOXL1zZdjpvZ4MHBT1lZW8rLKwAycjdBQ8Xj8O23MRYujEVNPImr/k8/\nLUh5B0+7dnH22aeGPfaoe8Xfs2fQxJPKmjXBTyrJZSEqj/pUHrUaWxbpAkqDgoKZ7Q90c/d/mNk4\ngjuKxrr7jM2s9hJwDTDBzPoDi929/hEcCDyWtJ/LgS/cfbKZ7UNQa2jYTdqy1eLxoGP7n/8s4uWX\nC3EvZOXKTU/8JSXxOif9xE+vXsGDTWrbF9lxNLSmcAcw3MyOIDiRXwzcCfwo3QruPtPMZpnZTKAG\nuMjMhgMr3X1KuFg3YFnSao8CfzOzC8K8nbs1ByNbVlMDs2YFgeCf/2zBggVBE05hYXDr5oABNfTs\nGa9zxd+1q078IvmioUFhnbt/bGYjgHvcfY414Nlqdx9TL+nDet/vW2/+S+CoBuZJGmj9enjjjUKm\nTi3ihReKWLYsCAQlJXFOPHEjxx9fxZAhVbRvn+WMikjWNTQotDGz04ChwLVm1gEozVy2pLEqKuCV\nV4rCpqGiaPC1jh1rOPPMDRx/fBVHHllN69zs1xaRLGloULgCGAX8zt1XmdlY4NaM5Uq2ydKlMV58\nMQgEM2YUsmFDEAh23bWGM8/cyI9/XMVBB1VTuP3drCUizaRBQcHdXzOzWWFA6AK8AryR2axJQyxc\nGGPq1KB/4N13C6IB3fbZp5rjj6/i+OOr2HvvGvUJiEiDNPTuoz8DH5jZFGAm8C7wU+D8DOZNUojH\n4cMPEx3FRcybF1z2FxTEGTAgCATHHVfFbrvl/jg2IpJ7Gtp89EN3vzi8K2iSu19rZq9kMmNSa+NG\nePPNwigQJIaHaNUqznHHJTqKq+nUSYFARBqnoUEh0fhwInBlOJ3+dVDSaGvWwGuvFTF1ahHTphVF\nzw+0axfntNOCQHDUUVW0aZPljIrIDqWhQWG+mc0heJjsAzMbBnyXwXzlpW++iTFtWiFTp7bg9dcL\nWbcuCATdu9dw6qlBIBgwoFojf4pIxjQ0KJxHMAbRnHB+NsFQ19JIixfHeOQReOKJ1rz9dmE0eNxe\ne9V2FO+3nzqKRaR5NDQotAZOAv5gZnGCgevGZyxXeeLJJ4u49NJWVFZCLFbIAQfUcPzxwa2jPXuq\nf0BEml9Dg8K9wJfABIL+haPDtJ9mKF87tKoquO66lvz1r8XstFOcP/8ZBg1aQ5cuCgQikl0NDQpd\n3P3/T5r/h5lNz0B+dnjLl8OIEa15/fUievWq4aGHKjn00DaUlysgiEj2NfTdj23MrCQxY2ZtgFaZ\nydKOa86cAo45pg2vv17EkCFVvPjiGnr33rHetCYi27eG1hQmAPPM7N1wfn/gqsxkacf03HNFXHxx\nK9aujXHppeu5/PINFOTP65hFZDvRoNOSu08EDgMeBCYBhwL9MpetHUdNDfzxj8Wce24w8tz991cy\nZowCgojkpga/ec3dvwC+SMyb2UEZydEOZNUquPDC1kybVsRuuwX9B337qrlIRHJXY17HqTvnN+Pj\njwsYNqw1CxYUMGhQFRMmVFKqwcZFJMc1phFDt8uk8eKLhRx7bAkLFhQwcuR6Jk9WQBCR7cNmawpm\n9gWpT/4xoFNGcrQdq6mB224r5sYbW9K6dZy7767klFOqsp0tEZEG21Lz0eHNkosdwOrVMHJkK6ZO\nbcH3vlfDgw9Wsu++6j8Qke3LZoOCuy9qroxszxYujHH22a1xL+Sww6q49951GsZaRLZLujGykV59\ntZBjj22DeyEjRmzgiScqFRBEZLvVmLuP8lo8DnfeWcy4ccW0aAF33FHJGWeo/0BEtm8KCttgzRr4\n1a9a8fTTLejWrYYHHqikf3/1H4jI9k9BYSt9/nnQfzB7diEHHVTF/fev0+imIrLDUJ/CVpgxo5Bj\njilh9uxCzj57A089VamAICI7FNUUGiAeh3vuacHYsS0pKICbb17HsGEbs50tEZEmp6CwBZWV8Jvf\ntOKJJ1rQuXMNEydWctBB6j8QkR2TgsJmfPVVjJ//vDUffFBI//7VPPBAJd26qblIRHZc6lNI4623\nChkypIQPPijkjDM28vTTaxXCrexfAAAKUklEQVQQRGSHp6BQTzwOkya14JRTWrN8eYzrr1/H7bev\no5XeMycieUDNR0nWr4ff/a4lf/tbMR071nDffes47LDqbGdLRKTZKCiEli6Ncc45rXnnnUL23bea\nSZMq6dFDzUUikl/UfATMmlXAkCElvPNOIaecspHnnlurgCAieSnvg8LkyUWcfHIJy5bFGDt2HXfd\ntY6SkmznSkQkO/K2+WjjRrj66pbcf38x7dvHueeeSgYNUv+BiOS3vA0Kl13WismTW9C3bzUPPljJ\n7ruruUhEJKNBwcxuAw4heKXnKHd/J0zfBXgkadGewBjgv4FJwG5ANfBzd1+Yibz17VvN8OFxrr56\nPTvtlIk9iIhsfzIWFMxsINDb3QeYWV9gIjAAwN2/AgaFyxUB04FngTOBFe5+lpkdA1wPnJ6J/F1w\ngcYuEhGpL5MdzYOBpwHcfS5QamY7p1huOPCku68O15kSpr8MHJbB/ImISD2ZbD7qCsxKmi8P01bV\nW+484JikdcoB3L3GzOJmVuzuG9LtpLS0hKKiwqbLdZaUlbXNdhZyhsqiLpVHXSqPWpkoi+bsaI7V\nTzCzAcA8d68fKNKuU9/y5Wsbm6+sKytrS3l5RbazkRNUFnWpPOpSedRqbFmkCyiZbD5aTHDln9Ad\nWFJvmRMJmok2WcfMWgCxzdUSRESkaWUyKLwEnApgZv2Bxe5eP6wdCHxYb53TwumTgNcymD8REakn\nY81H7j7TzGaZ2UygBrjIzIYDK9090ZncDViWtNrjwBAz+xewnqATWkREmklG+xTcfUy9pA/rfb9v\nvflq4OeZzJOIiKSX92MfiYhILQUFERGJKCiIiEhEQUFERCIKCiIiElFQEBGRiIKCiIhEFBRERCSi\noCAiIhEFBRERiSgoiIhIREFBREQiCgoiIhJRUBARkYiCgoiIRBQUREQkoqAgIiIRBQUREYkoKIiI\nSERBQUREIgoKIiISUVAQEZGIgoKIiEQUFEREJKKgICIiEQUFERGJKCiIiEhEQUFERCIKCiIiElFQ\nEBGRiIKCiIhEFBRERCSioCAiIhEFBRERiSgoiIhIpCiTGzez24BDgDgwyt3fSfquBzAZKAbec/cL\nzGwQ8N/A7HCxf7v7xZnMo4iI1MpYUDCzgUBvdx9gZn2BicCApEVuAW5x9ylm9hcz2zVMf93dT81U\nvkREJL1MNh8NBp4GcPe5QKmZ7QxgZgXAEcCz4fcXufvnGcyLiIg0QCabj7oCs5Lmy8O0VUAZUAHc\nZmb9gRnufkW4XD8zexboAFzj7tM2t5PS0hKKigqbPPPNraysbbazkDNUFnWpPOpSedTKRFlktE+h\nnli96V2A24HPgOfN7ATgA+Aa4AmgJ/Came3p7hvSbXT58rUZy3BzKStrS3l5RbazkRNUFnWpPOpS\nedRqbFmkCyiZDAqLCWoGCd2BJeH0N8Aid18AYGavAHu7+/PA4+EyC8zsa4Lg8WkG8ykiIqFM9im8\nBJwKEDYRLXb3CgB3rwIWmlnvcNn9ATezs8zssnCdrkAX4KsM5lFERJJkrKbg7jPNbJaZzQRqgIvM\nbDiw0t2nAKOBSWGn87+B54A2wKNmdjLBraoXbq7pSEREmlZG+xTcfUy9pA+TvvsEOLze9xXASZnM\nk4iIpKcnmkVEJKKgICIiEQUFERGJKCiIiEhEQUFERCIKCiIiElFQEBGRiIKCiIhEFBRERCSioCAi\nIhEFBRERiSgoiIhIJC+DwpQpRQwcWEK3bjsxcGAJU6Y057uGRERyV96dDadMKeL881tH83PnFobz\nlQwdWpW9jImI5IC8qymMH1+cMv3221Oni4jkk7wLCvPnpz7kdOkiIvkk786EffrUbFW6iEg+ybug\nMHp06rd7jhqlt36KiORdUBg6tIoJEyrp16+aoqI4/fpVM2GCOplFRCAP7z6CIDAoCIiIbCrvagoi\nIpKegoKIiEQUFEREJKKgICIiEQUFERGJxOLxeLbzICIiOUI1BRERiSgoiIhIREFBREQiCgoiIhJR\nUBARkYiCgoiIRBQUREQkkpejpOYSM/sTcATB7+J6d38qy1nKKjNrDXwEXOvuk7Kcnawys7OAy4Eq\n4Gp3fz7LWcoKM9sJeAgoBVoC17j7i9nNVXaY2T7AM8Bt7n6nmfUA/gYUAkuAn7n7+sbsQzWFLDKz\no4B93H0AcBwwPstZygVXAt9lOxPZZmYdgd8DhwMnAidnN0dZNRxwdz8KOBW4PbvZyQ4zawP8GXgl\nKfkPwF/c/QjgE+Ccxu5HQSG7/gc4LZxeAbQxs8Is5ierzGwvoB+Ql1fE9RwNvOzuFe6+xN1HZDtD\nWfQN0DGcLg3n89F64MfA4qS0QcCz4fRzBH83jaKgkEXuXu3ua8LZc4Gp7l6dzTxl2S3ApdnORI7Y\nHSgxs2fNbIaZDc52hrLF3R8DdjWzTwgupC7Lcpaywt2r3L2yXnKbpOaiZUC3xu5HQSEHmNnJBEFh\nZLbzki1mNgx4090/zXZeckSM4Or4FILmkwfMLJbVHGWJmf0U+Nzd9wR+BNyZ5Szlqib5+1BQyDIz\nOxb4L+B4d1+Z7fxk0QnAyWb2FnAecJWZNboqvB1bCswMrw4XABVAWZbzlC2HAS8CuPuHQPd8bmat\nZ3V4cwbALtRtWtomuvsoi8ysHXATcLS753Xnqrufnpg2s7HAZ+7+cvZylHUvAZPM7EaCdvSdyN+2\n9E+Ag4EnzWw3YHWeN7Mmexn4/4CHw88XGrtBBYXsOh3oBDxhZom0Ye7+efayJLnA3b8ys78Db4VJ\nF7t7TTbzlEUTgIlm9jrBOeuCLOcnK8xsf4J+t92BjWZ2KnAWwcXD+cAi4MHG7kfvUxARkYj6FERE\nJKKgICIiEQUFERGJKCiIiEhEQUFERCK6JVUkBTPbHXDgzXpfPe/uNzXB9gcB17n74Y3dlkhTUlAQ\nSa/c3QdlOxMizUlBQWQrmVkVcC1wFMGTxsPd/SMzO5jg4aKNQBwY6e5zzKw3cC9Bc+064OfhpgrN\n7C7ghwQjYJ4Qpj9K8BRzC+A5dx/XPEcmoj4FkW1RCHwU1iLuIhjTHoIXwfwqHPf/VuAvYfrdwE3u\nfiQwkdrh0vsCY939EIJAciwwBGgRjo9/KMHYNvo/lWajmoJIemVmNr1e2uXhZ+LNX28AvzGz9kAX\nd38nTJ8OPBZOHxzOJ4aBTvQpzHP3peEyXwLtCcbE/4OZPQFMBe7L4+EtJAsUFETSS9mnEI5Tlbh6\njxE0FdUfLyaWlBYnda28qv467r7MzPYDBhC8be1dM+ufYhx9kYxQtVRk2/wo/Dwc+L9w2PMlYb8C\nBG/ASgxmN5PgdauY2elm9sd0GzWzY4AT3P0Nd78cWA10zsQBiKSimoJIeqmajxIvAfqhmV1I0CE8\nLEwbBtxqZtVANXBhmD4SuMfMLiLoOzgH6JVmnw48aGaXh9t4yd0XNcXBiDSERkkV2UpmFifoDK7f\n/COy3VPzkYiIRFRTEBGRiGoKIiISUVAQEZGIgoKIiEQUFEREJKKgICIikf8Hkk2mCO540PIAAAAA\nSUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f30ce537150>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "Va0izhlL4Wyv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "With a higher number of words considered per review the validation accuracy can increase up to 88%."
      ]
    },
    {
      "metadata": {
        "id": "Ox3LYUPzoTaH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# In order to deal with the general hyper parameter like max_features(# of possible tokens), maxlen(# of words considered per input), validation_split=0.2\n",
        "\n",
        "# a general build model function is created"
      ]
    },
    {
      "metadata": {
        "id": "hkTEw9e6oPaJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def build_model(x_train,y_train,max_features = 10000,maxlen = 200,epochs=10,batch_size=32,validation_split=0.2):\n",
        "  \n",
        "  from keras.models import Sequential\n",
        "  from keras.layers import Flatten, Dense\n",
        "  # Number of words to consider as features\n",
        "  #max_features = 10000\n",
        "  # Cut texts after this number of words \n",
        "  # (among top max_features most common words)\n",
        "  #maxlen = 200\n",
        "\n",
        "  # Load the data as lists of integers.\n",
        "  #(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "\n",
        "  # This turns our lists of integers\n",
        "  # into a 2D integer tensor of shape `(samples, maxlen)`\n",
        "  x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "  #x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "  \n",
        "  \n",
        "  # The Embedding layer takes at least two arguments:\n",
        "  # the number of possible tokens, here 1000 (1 + maximum word index),\n",
        "  # and the dimensionality of the embeddings, here 64.\n",
        "  #embedding_layer = Embedding(1000, 64)\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(10000, 8, input_length=maxlen)) # the dimensionality of the embeddings, here 8. \n",
        "  model.add(Flatten())\n",
        "\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "  model.summary()\n",
        "\n",
        "  history = model.fit(x_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=32,\n",
        "                    validation_split=0.2)\n",
        "  return history,model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "snDRPllq4Wyw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Using pre-trained word embeddings\n",
        "\n",
        "\n",
        "When you have very little training data you can leverage pre-computed embeddings that are learnt on a different dataset. The \n",
        "rationale behind using pre-trained word embeddings in natural language processing is very much the same as for using pre-trained convnets \n",
        "in image classification: we don't have enough data available to learn truly powerful features on our own, but we expect the features that \n",
        "we need to be fairly generic, i.e. common visual features or semantic features. In this case it makes sense to reuse features learned on a \n",
        "different problem.\n",
        "\n",
        "There are various pre-computed databases of word embeddings that you can download and start using in a Keras `Embedding` layer. Word2Vec is one \n",
        "of them. Another popular one is called \"GloVe\", developed by Stanford researchers in 2014. It stands for \"Global Vectors for Word \n",
        "Representation\", and it is an embedding technique based on factorizing a matrix of word co-occurrence statistics. Its developers have made \n",
        "available pre-computed embeddings for millions of English tokens, obtained from Wikipedia data or from Common Crawl data.\n",
        "\n",
        "Let's take a look at how you can get started using GloVe embeddings in a Keras model. The same method will of course be valid for Word2Vec \n",
        "embeddings or any other word embedding database that you can download."
      ]
    },
    {
      "metadata": {
        "id": "7-sEBgNH4Wyx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Putting it all together: from raw text to word embeddings\n",
        "\n",
        "\n",
        "We will be using a model similar to the one we just went over -- embedding sentences in sequences of vectors, flattening them and training a \n",
        "`Dense` layer on top. But we will do it using pre-trained word embeddings, and instead of using the pre-tokenized IMDB data packaged in \n",
        "Keras, we will start from scratch, by downloading the original text data."
      ]
    },
    {
      "metadata": {
        "id": "0hwrmCij4Wyx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Download the IMDB data as raw text\n",
        "\n",
        "\n",
        "First, head to `http://ai.stanford.edu/~amaas/data/sentiment/` and download the raw IMDB dataset (if the URL isn't working anymore, just \n",
        "Google \"IMDB dataset\"). Uncompress it.\n",
        "\n",
        "Now let's collect the individual training reviews into a list of strings, one string per review, and let's also collect the review labels \n",
        "(positive / negative) into a `labels` list:"
      ]
    },
    {
      "metadata": {
        "id": "pPesPpMt4Wyy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "imdb_dir = '/Users/anjalisridhar/kdd2018/workshop/datasets/aclImdb'\n",
        "train_dir = os.path.join(imdb_dir, 'train')\n",
        "\n",
        "labels = []\n",
        "texts = []\n",
        "\n",
        "for label_type in ['neg', 'pos']:\n",
        "    dir_name = os.path.join(train_dir, label_type)\n",
        "    for fname in os.listdir(dir_name):\n",
        "        if fname[-4:] == '.txt':\n",
        "            f = open(os.path.join(dir_name, fname))\n",
        "            texts.append(f.read())\n",
        "            f.close()\n",
        "            if label_type == 'neg':\n",
        "                labels.append(0)\n",
        "            else:\n",
        "                labels.append(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gEtHLQ5C4Wy0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Tokenize the data\n",
        "\n",
        "\n",
        "Let's vectorize the texts we collected, and prepare a training and validation split.\n",
        "We will merely be using the concepts we introduced earlier in this section.\n",
        "\n",
        "Because pre-trained word embeddings are meant to be particularly useful on problems where little training data is available (otherwise, \n",
        "task-specific embeddings are likely to outperform them), we will add the following twist: we restrict the training data to its first 200 \n",
        "samples.\n"
      ]
    },
    {
      "metadata": {
        "id": "GF-MRlRU4Wy0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "maxlen = 100  # We will cut reviews after 100 words\n",
        "training_samples = 200  # We will be training on 200 samples\n",
        "validation_samples = 10000  # We will be validating on 10000 samples\n",
        "max_words = 10000  # We will only consider the top 10,000 words in the dataset\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "labels = np.asarray(labels)\n",
        "print('Shape of data tensor:', data.shape)\n",
        "print('Shape of label tensor:', labels.shape)\n",
        "\n",
        "# Split the data into a training set and a validation set\n",
        "# But first, shuffle the data, since we started from data\n",
        "# where sample are ordered (all negative first, then all positive).\n",
        "indices = np.arange(data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "\n",
        "x_train = data[:training_samples]\n",
        "y_train = labels[:training_samples]\n",
        "x_val = data[training_samples: training_samples + validation_samples]\n",
        "y_val = labels[training_samples: training_samples + validation_samples]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kp7zMVSy4Wy2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Download the GloVe word embeddings\n",
        "\n",
        "\n",
        "Head to `https://nlp.stanford.edu/projects/glove/` (where you can learn more about the GloVe algorithm), and download the pre-computed \n",
        "embeddings from 2014 English Wikipedia. It's a 822MB zip file named `glove.6B.zip`, containing 100-dimensional embedding vectors for \n",
        "400,000 words (or non-word tokens). Un-zip it."
      ]
    },
    {
      "metadata": {
        "id": "FfHzZtRP4Wy2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Pre-process the embeddings\n",
        "\n",
        "\n",
        "Let's parse the un-zipped file (it's a `txt` file) to build an index mapping words (as strings) to their vector representation (as number \n",
        "vectors)."
      ]
    },
    {
      "metadata": {
        "id": "qjgtBFpU4Wy2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "glove_dir = '/Users/anjalisridhar/kdd2018/workshop/datasets/glove.6B'\n",
        "\n",
        "embeddings_index = {}\n",
        "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qAf_CQEf4Wy5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "Now let's build an embedding matrix that we will be able to load into an `Embedding` layer. It must be a matrix of shape `(max_words, embedding_dim)`, where each entry `i` contains the `embedding_dim`-dimensional vector for the word of index `i` in our reference word index \n",
        "(built during tokenization). Note that the index `0` is not supposed to stand for any word or token -- it's a placeholder."
      ]
    },
    {
      "metadata": {
        "id": "_r9lyAnX4Wy5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "embedding_dim = 100\n",
        "\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if i < max_words:\n",
        "        if embedding_vector is not None:\n",
        "            # Words not found in embedding index will be all-zeros.\n",
        "            embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4ioiyiQH4Wy7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define a model\n",
        "\n",
        "We will be using the same model architecture as before:"
      ]
    },
    {
      "metadata": {
        "id": "I3grcIZc4Wy7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Flatten, Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XIQISoXA4Wy8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Load the GloVe embeddings in the model\n",
        "\n",
        "\n",
        "The `Embedding` layer has a single weight matrix: a 2D float matrix where each entry `i` is the word vector meant to be associated with \n",
        "index `i`. Simple enough. Let's just load the GloVe matrix we prepared into our `Embedding` layer, the first layer in our model:"
      ]
    },
    {
      "metadata": {
        "id": "tnGuNQdR4Wy9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.layers[0].set_weights([embedding_matrix])\n",
        "model.layers[0].trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3QI8vT9D4Wy_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "Additionally, we freeze the embedding layer (we set its `trainable` attribute to `False`), following the same rationale as what you are \n",
        "already familiar with in the context of pre-trained convnet features: when parts of a model are pre-trained (like our `Embedding` layer), \n",
        "and parts are randomly initialized (like our classifier), the pre-trained parts should not be updated during training to avoid forgetting \n",
        "what they already know. The large gradient update triggered by the randomly initialized layers would be very disruptive to the already \n",
        "learned features."
      ]
    },
    {
      "metadata": {
        "id": "S9va9qhk4Wy_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Train and evaluate\n",
        "\n",
        "Let's compile our model and train it:"
      ]
    },
    {
      "metadata": {
        "id": "WO_mnWn34Wy_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=32,\n",
        "                    validation_data=(x_val, y_val))\n",
        "model.save_weights('pre_trained_glove_model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pj5jgOaw4WzB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's plot its performance over time:"
      ]
    },
    {
      "metadata": {
        "id": "qWCsay7X4WzC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S_2jS7IK4WzE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "The model quickly starts overfitting, unsurprisingly given the small number of training samples. Validation accuracy has high variance for \n",
        "the same reason, but seems to reach high 50s.\n",
        "\n",
        "Note that your mileage may vary: since we have so few training samples, performance is heavily dependent on which exact 200 samples we \n",
        "picked, and we picked them at random. If it worked really poorly for you, try picking a different random set of 200 samples, just for the \n",
        "sake of the exercise (in real life you don't get to pick your training data).\n",
        "\n",
        "We can also try to train the same model without loading the pre-trained word embeddings and without freezing the embedding layer. In that \n",
        "case, we would be learning a task-specific embedding of our input tokens, which is generally more powerful than pre-trained word embeddings \n",
        "when lots of data is available. However, in our case, we have only 200 training samples. Let's try it:"
      ]
    },
    {
      "metadata": {
        "id": "4PT7fyXj4WzF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Flatten, Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=32,\n",
        "                    validation_data=(x_val, y_val))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7Uz5lwMO4WzG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pWQCuAx64WzI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "Validation accuracy stalls in the low 50s. So in our case, pre-trained word embeddings does outperform jointly learned embeddings. If you \n",
        "increase the number of training samples, this will quickly stop being the case -- try it as an exercise.\n",
        "\n",
        "Finally, let's evaluate the model on the test data. First, we will need to tokenize the test data:"
      ]
    },
    {
      "metadata": {
        "id": "aixv_TsM4WzI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_dir = os.path.join(imdb_dir, 'test')\n",
        "\n",
        "labels = []\n",
        "texts = []\n",
        "\n",
        "for label_type in ['neg', 'pos']:\n",
        "    dir_name = os.path.join(test_dir, label_type)\n",
        "    for fname in sorted(os.listdir(dir_name)):\n",
        "        if fname[-4:] == '.txt':\n",
        "            f = open(os.path.join(dir_name, fname))\n",
        "            texts.append(f.read())\n",
        "            f.close()\n",
        "            if label_type == 'neg':\n",
        "                labels.append(0)\n",
        "            else:\n",
        "                labels.append(1)\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "x_test = pad_sequences(sequences, maxlen=maxlen)\n",
        "y_test = np.asarray(labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CqvBbpET4WzJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "And let's load and evaluate the first model:"
      ]
    },
    {
      "metadata": {
        "id": "hKvomdlT4WzK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.load_weights('pre_trained_glove_model.h5')\n",
        "model.evaluate(x_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jaYPZDTn4WzL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We get an appalling test accuracy of ~55%."
      ]
    },
    {
      "metadata": {
        "id": "9eITjhSI4WzL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## A first recurrent layer in Keras"
      ]
    },
    {
      "metadata": {
        "id": "DUrbHmS54WzM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import SimpleRNN"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pIQl00so4WzN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "`SimpleRNN` processes batches of sequences, like all other Keras layers, not just a single sequence.\n",
        "Like all recurrent layers in Keras, `SimpleRNN` can be run in two different modes: it can return either the full sequences of successive \n",
        "outputs for each timestep (a 3D tensor of shape `(batch_size, timesteps, output_features)`), or it can return only the last output for each \n",
        "input sequence (a 2D tensor of shape `(batch_size, output_features)`). These two modes are controlled by the `return_sequences` constructor \n",
        "argument. Let's take a look at an example:"
      ]
    },
    {
      "metadata": {
        "id": "Xuo_5des4WzO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Exercise 2: Lets add a SimpleRNN layer to our model with embedding dimension 32"
      ]
    },
    {
      "metadata": {
        "id": "XxD6JdRt4WzO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, SimpleRNN\n",
        "\n",
        "model = Sequential()\n",
        "# Add an Embedding layer of 10000 vocab size(or max features) and 32 dimensions\n",
        "# ...\n",
        "# Add a SimpleRNN layer of output 32 dimensions\n",
        "# ..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D6wybuWt4WzP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "It is sometimes useful to stack several recurrent layers one after the other in order to increase the representational power of a network. \n",
        "In such a setup, you have to get all intermediate layers to return full sequences:"
      ]
    },
    {
      "metadata": {
        "id": "5nAp_4Y44WzP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Let us add 3 more SimpleRNN layers. This time we also want to set the \n",
        "# `return_sequences` parameter to be True.\n",
        "# This will return the output for each timestep as opposed to returning the output \n",
        "# for only the last timestep.\n",
        "# Compare model.summary() to see this difference.\n",
        "model = Sequential()\n",
        "model.add(Embedding(10000, 32))\n",
        "model.add(SimpleRNN(32, return_sequences=True))\n",
        "model.add(SimpleRNN(32, return_sequences=True))\n",
        "model.add(SimpleRNN(32, return_sequences=True))\n",
        "model.add(SimpleRNN(32))  # This last layer only returns the last outputs.\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-oBhJoxl4WzR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now let's try to use such a model on the IMDB movie review classification problem. First, let's preprocess the data:"
      ]
    },
    {
      "metadata": {
        "id": "P0GRd1Gx4WzR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "\n",
        "max_features = 10000  # number of words to consider as features\n",
        "maxlen = 500  # cut texts after this number of words (among top max_features most common words)\n",
        "batch_size = 32\n",
        "\n",
        "print('Loading data...')\n",
        "(input_train, y_train), (input_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "print(len(input_train), 'train sequences')\n",
        "print(len(input_test), 'test sequences')\n",
        "\n",
        "print('Pad sequences (samples x time)')\n",
        "input_train = sequence.pad_sequences(input_train, maxlen=maxlen)\n",
        "input_test = sequence.pad_sequences(input_test, maxlen=maxlen)\n",
        "print('input_train shape:', input_train.shape)\n",
        "print('input_test shape:', input_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LZj0S0YN4WzS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's train a simple recurrent network using an `Embedding` layer and a `SimpleRNN` layer:"
      ]
    },
    {
      "metadata": {
        "id": "jOdXnDqe4WzS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 32))\n",
        "model.add(SimpleRNN(32))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "history = model.fit(input_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=128,\n",
        "                    validation_split=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jzspDbdz4WzU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's display the training and validation loss and accuracy:"
      ]
    },
    {
      "metadata": {
        "id": "L0o1FyyN4WzU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5tUIEWf84WzV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Unfortunately, our small \n",
        "recurrent network doesn't perform very well at all compared to this baseline (only up to 85% validation accuracy). Part of the problem is \n",
        "that our inputs only consider the first 500 words rather the full sequences -- \n",
        "hence our RNN has access to less information than our earlier baseline model. The remainder of the problem is simply that `SimpleRNN` isn't very good at processing long sequences, like text. Other types of recurrent layers perform much better. Let's take a look at some \n",
        "more advanced layers."
      ]
    },
    {
      "metadata": {
        "id": "fwNWcAH34WzV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## A concrete LSTM example in Keras\n",
        "Now let's switch to more practical concerns: we will set up a model using a LSTM layer and train it on the IMDB data. Here's the network, \n",
        "similar to the one with `SimpleRNN` that we just presented. We only specify the output dimensionality of the LSTM layer, and leave every \n",
        "other argument (there are lots) to the Keras defaults. Keras has good defaults, and things will almost always \"just work\" without you \n",
        "having to spend time tuning parameters by hand."
      ]
    },
    {
      "metadata": {
        "id": "6tkXSQYe4WzW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Exercise 3: Use an LSTM layer instead of a SimpleRNN layer"
      ]
    },
    {
      "metadata": {
        "id": "ecM6xaH84WzW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import LSTM\n",
        "\n",
        "model = Sequential()\n",
        "# Add an Embedding layer as before with 10000 vocab size(max features) and 32 output dimensions\n",
        "# ...\n",
        "# Add a LSTM layer of 32 dimensions\n",
        "# ...\n",
        "\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])\n",
        "history = model.fit(input_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=128,\n",
        "                    validation_split=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yeu6K3JA4WzY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pWLUMCVe4WzZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You can see that the accuracy is 87%, much higher than what we got with a SimpleRNN layer. "
      ]
    },
    {
      "metadata": {
        "id": "bvCDZpjI4WzZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Exercise 1: Solution\n",
        "# model.add(Embedding(10000, 8, input_length=maxlen))\n",
        "# model.add(Flatten())\n",
        "# model.add(Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QG9P9GXd4Wzb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Exercise 2: Solution\n",
        "# model.add(Embedding(10000, 32))\n",
        "# model.add(SimpleRNN(32))\n",
        "# model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k_TrYEUj4Wzc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Exercise 3: Solution\n",
        "# model.add(Embedding(max_features, 32))\n",
        "# model.add(LSTM(32))\n",
        "# model.add(Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QtKkFjuq8CMc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# a day long practice in CDSW following two example here"
      ]
    },
    {
      "metadata": {
        "id": "4gdLNTYr790b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Import Keras and verify that the TensorFlow backend is set as the default.\n",
        "!pip3 install keras\n",
        "!pip3 install tensorflow\n",
        "import keras\n",
        "keras.__version__\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\"\"\"\n",
        "import scipy\n",
        "print('scipy: %s' % scipy.__version__)\n",
        "# numpy\n",
        "import numpy\n",
        "print('numpy: %s' % numpy.__version__)\n",
        "# matplotlib\n",
        "import matplotlib\n",
        "print('matplotlib: %s' % matplotlib.__version__)\n",
        "# pandas\n",
        "import pandas\n",
        "print('pandas: %s' % pandas.__version__)\n",
        "# statsmodels\n",
        "import statsmodels\n",
        "print('statsmodels: %s' % statsmodels.__version__)\n",
        "# scikit-learn\n",
        "import sklearn\n",
        "print('sklearn: %s' % sklearn.__version__)\n",
        "\n",
        "!conda update scikit-learn\n",
        "\n",
        "!pip3 install scikit-learn\n",
        "\"\"\"\n",
        "\n",
        "## datasets\n",
        "\n",
        "\"\"\"\n",
        "Dataset\n",
        "We will be working with the Reuters dataset, \n",
        "a set of short newswires and their topics, \n",
        "published by Reuters in 1986. \n",
        "It's a very simple, widely used toy dataset for text classification. \n",
        "There are 46 different topics; some topics are more represented than others, but each topic has at least 10 examples in the training set.\n",
        "\n",
        "A number of datasets come packaged as part of Keras. A few examples of these Datasets can be found here. Some example datasets are IMDB, MNIST, CIFAR10 etc. The Reuters dataset also comes prepackaged with Keras. Lets take a look at what the data looks like.\n",
        "\"\"\"\n",
        "\n",
        "from keras.datasets import reuters\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)\n",
        "\n",
        "## The argument num_words=10000 restricts the data to the 10,000 most frequently occurring words found in the data.\n",
        "\n",
        "## have 8,982 training examples and 2,246 test examples:\n",
        "\n",
        "8982+2246\n",
        "\n",
        "len(train_data)\n",
        "\n",
        "len(test_data)\n",
        "\n",
        "train_data.shape,test_data.shape\n",
        "\n",
        "type(train_data),train_data.shape\n",
        "\n",
        "## Each example is a list of integers (word indices):\n",
        "\n",
        "train_data[10]\n",
        "\n",
        "## Here's how you can decode it back to words, in case you are curious:\n",
        "\n",
        "word_index = reuters.get_word_index()\n",
        "\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "\n",
        "len(reverse_word_index) # 30,979 unique words in the reuters datasets\n",
        "\n",
        "reverse_word_index.get(0)\n",
        "reverse_word_index.get(1)\n",
        "reverse_word_index.get(2)\n",
        "\n",
        "train_data[0]\n",
        "train_data[10]\n",
        "\n",
        "\n",
        "\n",
        "decoded_newswire_train_data_10 = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])\n",
        "\n",
        "decoded_newswire_train_data_10\n",
        "\n",
        "decoded_newswire_train_data_10_n = ' '.join([reverse_word_index.get(i - 2, '?') for i in train_data[0]])\n",
        "\n",
        "decoded_newswire_train_data_10_n\n",
        "\n",
        "\n",
        "# Note that our indices were offset by 3\n",
        "\n",
        "# because 0, 1 and 2 are reserved indices for \"padding\", \"start of sequence\", and \"unknown\".\n",
        "\n",
        "decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])\n",
        "\n",
        "decoded_newswire\n",
        "\n",
        "\n",
        "#Since we limit the number of word index dictionary to the first 10000 words, there is a chance that the newswire will have words that do not have an index associated with it. This is what the ? symbol represents in the above newswire.\n",
        "\n",
        "#The label associated with an example is an integer between 0 and 45: a topic index.\n",
        "\n",
        "train_labels[10]\n",
        "\n",
        "train_labels[0]\n",
        "\n",
        "train_labels.shape\n",
        "\n",
        "type(train_labels)\n",
        "\n",
        "unique, counts = np.unique(train_labels, return_counts=True)\n",
        "\n",
        "print (np.asarray((unique, counts)).T)\n",
        "\n",
        "type(counts)\n",
        "\n",
        "counts.sum()\n",
        "\n",
        "Preparing the data\n",
        "\n",
        "We can vectorize the data with the exact same code as in our previous example:\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "results = np.zeros((len(train_data), 10000))\n",
        "type(results), results.shape\n",
        "\n",
        "type(train_data)\n",
        "\n",
        "test_train_data = train_data[:2]\n",
        "\n",
        "type(test_train_data),test_train_data.shape\n",
        "\n",
        "type(train_data),train_data.shape\n",
        "\n",
        "results = np.zeros((len(test_train_data), 10000))\n",
        "\n",
        "results\n",
        "\n",
        "  for i, data in enumerate(test_train_data):\n",
        "    print(\"line %s\" %i)\n",
        "    print(i,data)\n",
        "\n",
        "    results[i, data] = 1.\n",
        "results.shape\n",
        "\n",
        "results[0][:18]\n",
        "\n",
        "results.shape\n",
        "\n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "\n",
        "    results = np.zeros((len(sequences), dimension))\n",
        "\n",
        "    for i, sequence in enumerate(sequences):\n",
        "\n",
        "        results[i, sequence] = 1.\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# Our vectorized training data\n",
        "\n",
        "x_train = vectorize_sequences(train_data)\n",
        "\n",
        "# Our vectorized test data\n",
        "\n",
        "x_test = vectorize_sequences(test_data)\n",
        "\n",
        "x_train[0][:10]\n",
        "\n",
        "\n",
        "To vectorize the labels, there are two possibilities: we could just cast the label list as an integer tensor, or we could use a \"one-hot\" encoding. One-hot encoding of our labels consists in embedding each label as an all-zero vector with a 1 in the place of the label index. You can do this using a built-in Keras function:\n",
        "\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "\n",
        "one_hot_train_labels = to_categorical(train_labels)\n",
        "\n",
        "type(one_hot_train_labels)\n",
        "one_hot_train_labels.size\n",
        "one_hot_train_labels.shape\n",
        "\n",
        "one_hot_train_labels.shape\n",
        "train_labels.shape\n",
        "\n",
        "one_hot_train_labels[:5]\n",
        "\n",
        "one_hot_test_labels = to_categorical(test_labels)\n",
        "\n",
        "one_hot_test_labels.shape\n",
        "\n",
        "Building our network\n",
        "\n",
        "In this topic classification problem we are trying to classify short snippets of text. \n",
        "\n",
        "The difference between a binary classification problem and this one is that the number of output class is 46 \n",
        "\n",
        "i.e the dimensionality of the output space is much larger. \n",
        "\n",
        "(A binary classification problem is one in which there are two possible output clases.)\n",
        "Build a Sequential model:\n",
        "\n",
        "from keras import models\n",
        "\n",
        "from keras import layers\n",
        "\n",
        "x_train.shape,x_test.shape\n",
        "\n",
        "# Instantiate a Sequential Model\n",
        "\n",
        "model = models.Sequential()\n",
        "\n",
        "# Add 2 Dense layers of 64 units each to the model. Let `relu` be the activation function.\n",
        "\n",
        "# Note: Specify the input_shape argument for the first layer.\n",
        "\n",
        "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
        "\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "\n",
        "# Add a final Dense layer that classifies the output\n",
        "\n",
        "# Note: You should use 46 units since out output dimension is going to be the number of output classes). \n",
        "\n",
        "# Let `softmax` be the activation function.\n",
        "\n",
        "model.add(layers.Dense(46, activation='softmax'))\n",
        "\n",
        "Some things to note about this architecture:\n",
        "\n",
        "    We are ending the network with a Dense layer of size 46. This means that for each input sample, \n",
        "    our network will output a 46-dimensional vector. Each entry in this vector (each dimension) will encode a different output class.\n",
        "    We are using the Relu function for activation.\n",
        "    The last layer uses a softmax activation. \n",
        "    It means that the network will output a probability distribution over the 46 different output classes, \n",
        "    i.e. for every input sample, the network will produce a 46-dimensional output vector \n",
        "    where output[i] is the probability that the sample belongs to class i. The 46 scores will sum to 1.\n",
        "\n",
        "Compile the model:\n",
        "\n",
        "Compiling the model is \"freezing\" the model with certain attributes set such as loss, optimizer, metrics etc.\n",
        "\n",
        "# Compile the model with a \"rmsprop\" optimizer, \"categorical_crossentropy\" loss and \"accuracy\" \\\n",
        "\n",
        "# metric.\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "\n",
        "              loss='categorical_crossentropy',\n",
        "\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "In Keras you can specify the different parameters above as strings such as \"rmsprop\" or as keras.optimizers.RMSprop. \n",
        "You can do the same for loss and metrics. Here are a list of optimizers, loss functions and metrics that are available in Keras.\n",
        "\n",
        "In our example above the best loss function to use categorical_crossentropy \n",
        "since it measures the distance between the probability distribution output by our network and the true distribution of the labels. \n",
        "\n",
        "By minimizing the distance between these two distributions, \n",
        "we train our network to output something as close as possible to the true labels.\n",
        "\n",
        "Another point to note is if we had used output labels as integer tensors instead of one-hot encoded vectors, \n",
        "then we should usesparse_categorical_crossentropy instead of categorical_crossentropy.\n",
        "Training our model\n",
        "\n",
        "Let's set apart 1,000 samples in our training data to use as a validation set:\n",
        "\n",
        "# Split the training data into train and validation datasets.\n",
        "\n",
        "x_val = x_train[:1000]\n",
        "\n",
        "partial_x_train = x_train[1000:]\n",
        "\n",
        "partial_x_train.shape\n",
        "\n",
        "\n",
        "\n",
        "y_val = one_hot_train_labels[:1000]\n",
        "\n",
        "partial_y_train = one_hot_train_labels[1000:]\n",
        "\n",
        "Now let's train our network for 20 epochs:\n",
        "\n",
        "# Use the `fit` call with the above training and validation datasets. \n",
        "\n",
        "# Use can use a batch size of 512.\n",
        "\n",
        "history = model.fit(partial_x_train,\n",
        "\n",
        "                    partial_y_train,\n",
        "\n",
        "                    epochs=20,\n",
        "\n",
        "                    batch_size=512,\n",
        "\n",
        "                    validation_data=(x_val, y_val))\n",
        "\n",
        "Let's display its loss and accuracy curves:\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "loss = history.history['loss']\n",
        "\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "\n",
        "\n",
        "def plot_fun(loss,val_loss):\n",
        "\n",
        "  epochs = range(1, len(loss) + 1)\n",
        "  plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "\n",
        "  plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "  plt.title('Training and validation loss')\n",
        "\n",
        "  plt.xlabel('Epochs')\n",
        "\n",
        "  plt.ylabel('Loss')\n",
        "\n",
        "  plt.legend()\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "#plt.clf()   # clear figure\n",
        "\n",
        "plot_fun(loss,val_loss)\n",
        "\n",
        "\n",
        "\n",
        "acc = history.history['acc']\n",
        "\n",
        "val_acc = history.history['val_acc']\n",
        "\n",
        "def plot_acc(loss,acc,val_acc):\n",
        "  \n",
        "  epochs = range(1, len(loss) + 1)\n",
        "\n",
        "  plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "\n",
        "  plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "\n",
        "  plt.title('Training and validation accuracy')\n",
        "\n",
        "  plt.xlabel('Epochs')\n",
        "\n",
        "  plt.ylabel('Loss')\n",
        "\n",
        "  plt.legend()\n",
        "\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "plot_acc(loss,acc,val_acc)\n",
        "\n",
        "## It seems that the network starts overfitting after 8 epochs.\n",
        "# Let's train a new network from scratch for 8 epochs, then let's evaluate it on the test set:\n",
        "\n",
        "# Use the same model from before and train the model as before. \n",
        "\n",
        "# However for this run let the number of epochs be 8.\n",
        "\n",
        "model = models.Sequential()\n",
        "\n",
        "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
        "\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "\n",
        "model.add(layers.Dense(46, activation='softmax'))\n",
        "\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "\n",
        "              loss='categorical_crossentropy',\n",
        "\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "history_8 = model.fit(partial_x_train,\n",
        "\n",
        "                    partial_y_train,\n",
        "\n",
        "                    epochs=8,\n",
        "\n",
        "                    batch_size=512,\n",
        "\n",
        "                    validation_data=(x_val, y_val))\n",
        "\n",
        "Evaluate the model:\n",
        "\n",
        "# Use `evaluate` the model with the test training input(x_test) and \n",
        "\n",
        "# the one-hot encoded labels(one_hot_test_labels).\n",
        "\n",
        "results = model.evaluate(x_test, one_hot_test_labels)\n",
        "\n",
        "results\n",
        "\n",
        "Our approach reaches an accuracy of ~78%. \n",
        "With a balanced binary classification problem, \n",
        "the accuracy reached by a purely random classifier would be 50%,\n",
        "but in our case it is closer to 19%, \n",
        "so our results seem pretty good, at least when compared to a random baseline:\n",
        "\n",
        "import copy\n",
        "\n",
        "\n",
        "\n",
        "test_labels_copy = copy.copy(test_labels)\n",
        "\n",
        "np.random.shuffle(test_labels_copy)\n",
        "\n",
        "float(np.sum(np.array(test_labels) == np.array(test_labels_copy))) / len(test_labels)\n",
        "\n",
        "Exercise 1: Now let us try with a larger network with 128 Dense units:\n",
        "\n",
        "# Use the same model as before but with 128 units set for the first two Dense layers. `compile`, `fit` and `evaluate`\n",
        "\n",
        "# the model as before. You can also plot the loss and accuracy to see the behavior of the model.\n",
        "\n",
        "\n",
        "\n",
        "# Define your model here\n",
        "\n",
        "# ...\n",
        "\n",
        "model = models.Sequential()\n",
        "\n",
        "model.add(layers.Dense(128, activation='relu', input_shape=(10000,)))\n",
        "\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "\n",
        "model.add(layers.Dense(46, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "\n",
        "              loss='categorical_crossentropy',\n",
        "\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "history_128 = model.fit(partial_x_train,\n",
        "\n",
        "          partial_y_train,\n",
        "\n",
        "          epochs=20,\n",
        "\n",
        "          batch_size=512,\n",
        "\n",
        "          validation_data=(x_val, y_val))\n",
        "\n",
        "results = model.evaluate(x_test, one_hot_test_labels)\n",
        "\n",
        "results\n",
        "\n",
        "\n",
        "\n",
        "plt.clf()   # clear figure\n",
        "\n",
        "def plt_all(history):\n",
        "\n",
        "  loss = history.history['loss']\n",
        "\n",
        "  val_loss = history.history['val_loss']\n",
        "  \n",
        "  plot_fun(loss,val_loss)\n",
        "\n",
        "\n",
        "  acc = history.history['acc']\n",
        "\n",
        "  val_acc = history.history['val_acc']\n",
        "\n",
        "  plot_acc(loss,acc,val_acc)\n",
        "\n",
        "plt_all(history_128)\n",
        "## As you can see, with the larger network we get a slight increase in accuracy \n",
        "but the network seems to perform just as good previously.\n",
        "Exercise 2: Now let us train the model with a single Dense layer network:\n",
        "\n",
        "# Use the same model as before but with 1 units set for a single Dense layer. `compile`, `fit` and `evaluate`\n",
        "\n",
        "# the model as before. You can also plot the loss and accuracy to see the behavior of the model.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define your model here\n",
        "\n",
        "# \n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(1, activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(46, activation='softmax'))\n",
        "model.compile(optimizer='rmsprop',\n",
        "\n",
        "              loss='categorical_crossentropy',\n",
        "\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "history_1 = model.fit(partial_x_train,\n",
        "\n",
        "          partial_y_train,\n",
        "\n",
        "          epochs=20,\n",
        "\n",
        "          batch_size=512,\n",
        "\n",
        "          validation_data=(x_val, y_val))\n",
        "\n",
        "results_1 = model.evaluate(x_test, one_hot_test_labels)\n",
        "print(results_1)\n",
        "\n",
        "plt_all(history_1)\n",
        "\n",
        "\n",
        "\n",
        "You can see that the accuracy stagnates at around 35% since the network is unable to learn any new information \n",
        "from the data.\n",
        "\n",
        "## Generating predictions on new data\n",
        "\n",
        "We can verify that the predict method of our model instance returns a probability distribution over all 46 topics.\n",
        "\n",
        "Let's generate topic predictions for all of the test data:\n",
        "\n",
        "predictions = model.predict(x_test)\n",
        "\n",
        "Each entry in predictions is a vector of length 46:\n",
        "\n",
        "predictions[0].shape\n",
        "\n",
        "predictions.shape\n",
        "\n",
        "The coefficients in this vector sum to 1:\n",
        "\n",
        "np.sum(predictions[0])\n",
        "\n",
        "The largest entry is the predicted class, i.e. the class with the highest probability:\n",
        "\n",
        "np.argmax(predictions[0])\n",
        "\n",
        "predictions[0]\n",
        "\n",
        "Dealing with overfitting:\n",
        "\n",
        "Common ways to deal with overfitting is to train the model on more data \n",
        "or even to reduce the capacity of the network. \n",
        "A bigger network gets its training loss near zero very quickly. \n",
        "The more capacity the network has, the quicker it will be able to model the training data \n",
        "(resulting in a low training loss), \n",
        "but the more susceptible it is to overfitting (resulting in a large difference between the training and validation loss). \n",
        "Other techniques to deal with overfitting is using weight regularization and dropout. \n",
        "Keras provides APIs that make it very simple to use these techniques.\n",
        "Adding weight regularization:\n",
        "\n",
        "A common way to mitigate overfitting is to put constraints on the complexity of a network by forcing its weights \n",
        "to only take small values, \n",
        "which makes the distribution of weight values more \"regular\". \n",
        "This is called \"weight regularization\", \n",
        "and it is done by adding to the loss function of the network a cost associated with having large weights. \n",
        "This cost comes in two flavors:\n",
        "\n",
        "L1 regularization, \n",
        "where the cost added is proportional to the absolute value of the weights coefficients \n",
        "(i.e. to what is called the \"L1 norm\" of the weights). \n",
        "\n",
        "L2 regularization, where the cost added is proportional to the square of the value of the weights coefficients \n",
        "(i.e. to what is called the \"L2 norm\" of the weights). \n",
        "L2 regularization is also called weight decay in the context of neural networks. \n",
        "\n",
        "Don't let the different name confuse you: weight decay is mathematically the exact same as L2 regularization. \n",
        "\n",
        "In Keras, weight regularization is added by passing weight regularizer instances to layers as keyword arguments. \n",
        "\n",
        "Let's add L2 weight regularization to an example Dense layer:\n",
        "\n",
        "from keras import regularizers\n",
        "\n",
        "mock_dense_layer = layers.Dense(64, kernel_regularizer=regularizers.l2(0.001), activation='relu', input_shape=(10000,))\n",
        "\n",
        "Adding dropout:\n",
        "\n",
        "Dropout, applied to a layer, \n",
        "consists of randomly \"dropping out\" \n",
        "(i.e. setting to zero) a number of output features of the layer during training. \n",
        "Let's say a given layer would normally have returned a vector [0.2, 0.5, 1.3, 0.8, 1.1] \n",
        "for a given input sample during training; after applying dropout,\n",
        "this vector will have a few zero entries distributed at random, e.g. [0, 0.5, 1.3, 0, 1.1]. \n",
        "The \"dropout rate\" is the fraction of the features that are being zeroed-out; \n",
        "it is usually set between 0.2 and 0.5. \n",
        "At test time, no units are dropped out, and instead the layer's output values are scaled down by a factor equal to the dropout rate,\n",
        "so as to balance for the fact that more units are active than at training time.\n",
        "\n",
        "# An example Dropout layer:\n",
        "\n",
        "#...\n",
        "\n",
        "mock_model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
        "\n",
        "mock_model.add(layers.Dropout(0.5)) # 0.5 is the dropout rate\n",
        "\n",
        "#...\n",
        "\n",
        "Callbacks\n",
        "\n",
        "You can use Keras built-in callbacks to checkpoint your model, save TensorBoard summaries, adjust learning rate during training etc. Callbacks are a way to perform an action, view internal state or model statistics at the beginning/end of the training loop, an epoch or a step in the fit loop. Let's instantiate the ModelCheckpoint and TensorBoard callback:\n",
        "Exercise 3: Create a ModelCheckpoint and TensorBoard callback.\n",
        "\n",
        "from keras import callbacks\n",
        "\n",
        "# Create a ModelCheckpoint callback by specifying a filepath such a s'/tmp/checkpoints'. \n",
        "# You can look at some of the\n",
        "\n",
        "# other parameters that you can set in the link above.\n",
        "\n",
        "# ...\n",
        "\n",
        "\n",
        "\n",
        "# \n",
        "\n",
        "# Create a ModelCheckpoint callback\n",
        "\n",
        "# model_checkpoint_cb = \n",
        "\n",
        "model_checkpoint_cb = callbacks.ModelCheckpoint('/tmp/checkpoints',\n",
        "                                                monitor='val_loss',\n",
        "                                                period=1)\n",
        "\n",
        "# Create a TensorBoard callback that writes model summaries to a given directory such as '/tmp/logs'.\n",
        "\n",
        "# tensorboard_cb = \n",
        "\n",
        "tensorboard_cb = callbacks.TensorBoard(log_dir='/tmp/logs')\n",
        "\n",
        "# Let us use our model from above:\n",
        "\n",
        "from keras import models\n",
        "\n",
        "from keras import layers\n",
        "\n",
        "\n",
        "model = models.Sequential()\n",
        "\n",
        "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
        "\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "\n",
        "model.add(layers.Dense(46, activation='softmax'))\n",
        "\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "\n",
        "              loss='categorical_crossentropy',\n",
        "\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# Pass the callbacks instantiated above to the `callbacks` parameter in the `fit` call.\n",
        "\n",
        "history_64 = model.fit(partial_x_train,\n",
        "\n",
        "          partial_y_train,\n",
        "\n",
        "          epochs=20,\n",
        "\n",
        "          batch_size=512,\n",
        "\n",
        "          validation_data=(x_val, y_val),\n",
        "\n",
        "          callbacks=[model_checkpoint_cb, tensorboard_cb])\n",
        "\n",
        "## You should be able to view TensorBoard summaries by using the following command: \n",
        "\n",
        "## tensorboard --logdir=/full_path_to_your_logs. Make sure to have installed the TensorBoard pip package.\n",
        "!ls /tmp/logs\n",
        "\n",
        "!ls /tmp/checkpoints\n",
        "!tensorboard --logdir=/tmp/logs\n",
        "# Exercise 1: Solution\n",
        "\n",
        "plt_all(history_64)\n",
        "\n",
        "# model = models.Sequential()\n",
        "\n",
        "# model.add(layers.Dense(128, activation='relu', input_shape=(10000,)))\n",
        "\n",
        "# model.add(layers.Dense(128, activation='relu'))\n",
        "\n",
        "# model.add(layers.Dense(46, activation='softmax'))\n",
        "\n",
        "# Exercise 2: Solution\n",
        "\n",
        "# model = models.Sequential()\n",
        "\n",
        "# model.add(layers.Dense(1, activation='relu', input_shape=(10000,)))\n",
        "\n",
        "# model.add(layers.Dense(46, activation='softmax'))\n",
        "\n",
        "# Exercise 3: Solution\n",
        "\n",
        "# model_checkpoint_cb = callbacks.ModelCheckpoint('/tmp/checkpoints',\n",
        "\n",
        "#                                                monitor='val_loss',\n",
        "\n",
        "#                                                period=1)\n",
        "\n",
        "\n",
        "\n",
        "# tensorboard_cb = callbacks.TensorBoard(log_dir='/tmp/logs')\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "Dataset\n",
        "Let's consider IMDB movie review sentiment prediction task that you are already familiar with. Let's quickly prepare the data. We will restrict the movie reviews to the top 10,000 most common words, and cut the reviews after only 20 words. Our network will simply learn 8-dimensional embeddings for each of the 10,000 words, turn the input integer sequences (2D integer tensor) into embedded sequences (3D float tensor), flatten the tensor to 2D, and train a single Dense layer on top for classification.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "from keras.datasets import imdb\n",
        "from keras import preprocessing\n",
        "\n",
        "# Number of words to consider as features\n",
        "imdb_max_features = 10000\n",
        "# Cut texts after this number of words \n",
        "# (among top max_features most common words)\n",
        "#imdb_maxlen = 20\n",
        "\n",
        "# Load the data as lists of integers.\n",
        "(imdb_x_train, imdb_y_train), (imdb_x_test, imdb_y_test) = imdb.load_data(num_words=imdb_max_features)\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten, Dense\n",
        "from keras.layers import Embedding\n",
        "\n",
        "model = Sequential()\n",
        "# Add an Embedding Layer with maximum number of tokes to be 10000 and embedding dimensionality as 8. \n",
        "# Let the input_length be the maximum length of each review i.e 20 as seen previously.\n",
        "# After the Embedding layer, \n",
        "# our activations have shape `(samples, maxlen, 8)`.\n",
        "\n",
        "# We flatten the 3D tensor of embeddings \n",
        "# into a 2D tensor of shape `(samples, maxlen * 8)`\n",
        "# ...\n",
        "# We add a Dense classifier on top\n",
        "# ...\n",
        "\n",
        "# Exercise 1: Solution\n",
        "maxlen = 20\n",
        "\n",
        "\"\"\"no padding \n",
        "will have this error\n",
        "ValueError: Error when checking input: expected embedding_2_input to have shape (20,) but got array with shape (1,)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "model.add(Embedding(10000, 8, input_length=maxlen))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "model.summary()\n",
        "\n",
        "history_20 = model.fit(imdb_x_train, imdb_y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=32,\n",
        "                    validation_split=0.2)\n",
        "\n",
        "\n",
        "def build_model(x_train,y_train,max_features = 10000,maxlen = 200,epochs=10,batch_size=32,validation_split=0.2):\n",
        "  \n",
        "  from keras.models import Sequential\n",
        "  from keras.layers import Flatten, Dense\n",
        "  # Number of words to consider as features\n",
        "  #max_features = 10000\n",
        "  # Cut texts after this number of words \n",
        "  # (among top max_features most common words)\n",
        "  #maxlen = 200\n",
        "\n",
        "  # Load the data as lists of integers.\n",
        "  #(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "\n",
        "  # This turns our lists of integers\n",
        "  # into a 2D integer tensor of shape `(samples, maxlen)`\n",
        "  x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "  #x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "  \n",
        "  \n",
        "  # The Embedding layer takes at least two arguments:\n",
        "  # the number of possible tokens, here 1000 (1 + maximum word index),\n",
        "  # and the dimensionality of the embeddings, here 64.\n",
        "  #embedding_layer = Embedding(1000, 64)\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(10000, 8, input_length=maxlen)) # the dimensionality of the embeddings, here 8. \n",
        "  model.add(Flatten())\n",
        "\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "  model.summary()\n",
        "\n",
        "  history_200 = model.fit(x_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=32,\n",
        "                    validation_split=0.2)\n",
        "  return history_200,model\n",
        "\n",
        "hist_20,model_20 = build_model(imdb_x_train,imdb_y_train,maxlen = 20)\n",
        "\n",
        "plt_all(hist_20)\n",
        "\n",
        "hist_200,model_200 = build_model(imdb_x_train,imdb_y_train,maxlen = 200)\n",
        "plt_all(hist_200)\n",
        "\n",
        "## Putting it all together: from raw text to word embeddings\n",
        "\n",
        "import os\n",
        "# /resources/data/imdb_raw/aclImdb\n",
        "imdb_dir = 'resources/data/imdb_raw/aclImdb'\n",
        "train_dir = os.path.join(imdb_dir, 'train')\n",
        "train_dir\n",
        "labels = []\n",
        "texts = []\n",
        "\n",
        "\n",
        "for label_type in ['neg', 'pos']:\n",
        "    dir_name = os.path.join(train_dir, label_type)\n",
        "    \n",
        "    print(dir_name)\n",
        "\n",
        "for label_type in ['neg', 'pos']:\n",
        "    dir_name = os.path.join(train_dir, label_type)\n",
        "    for fname in os.listdir(dir_name):\n",
        "        if fname[-4:] == '.txt':\n",
        "            f = open(os.path.join(dir_name, fname))\n",
        "            texts.append(f.read())\n",
        "            f.close()\n",
        "            if label_type == 'neg':\n",
        "                labels.append(0)\n",
        "            else:\n",
        "                labels.append(1)\n",
        "len(texts),len(labels)\n",
        "\n",
        "print('Shape of data :', len(texts))\n",
        "print('Shape of label tensor:', len(labels))\n",
        "\n",
        "labels[1],texts[1]\n",
        "\n",
        "for i in range(10):\n",
        "  print(\"line %s review --- it is a %d ------\" %(i,labels[i]))\n",
        "  print(texts[i])\n",
        "\n",
        "import numpy as np\n",
        "key,values = np.unique(labels,return_counts=True)\n",
        "\n",
        "key,values\n",
        "\n",
        "## Tokenize the data\n",
        "Let's vectorize the texts we collected, and prepare a training and validation split. \n",
        "We will merely be using the concepts we introduced earlier in this section.\n",
        "\n",
        "Because pre-trained word embeddings are meant to be particularly useful on problems \n",
        "where little training data is available \n",
        "(otherwise, task-specific embeddings are likely to outperform them), \n",
        "we will add the following twist: we restrict the training data to its first 200 samples.\n",
        "  \n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "maxlen = 100  # We will cut reviews after 100 words\n",
        "training_samples = 200  # We will be training on 200 samples\n",
        "validation_samples = 10000  # We will be validating on 10000 samples\n",
        "max_words = 10000  # We will only consider the top 10,000 words in the dataset\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words) #\n",
        "\"\"\"\n",
        "num_words: the maximum number of words to keep, \n",
        "based on word frequency. Only the most common num_words words will be kept.\n",
        "\n",
        "The Tokenizer stores everything in the word_index during fit_on_texts. \n",
        "Then, when calling the texts_to_sequences method, only the top num_words are considered.\n",
        "\"\"\"\n",
        "test_texts3 = ['a a a', 'b b', 'c',\"d d d d\"]\n",
        "test_tokenizer = Tokenizer(num_words=2)\n",
        "test_tokenizer.fit_on_texts(test_texts)\n",
        "\n",
        "test_tokenizer.word_index\n",
        "\n",
        "test_sequence = test_tokenizer.texts_to_sequences(test_texts)\n",
        "test_sequence\n",
        "\n",
        "test_tokenizer3 = Tokenizer(num_words=3)\n",
        "test_tokenizer3.fit_on_texts(test_texts3)\n",
        "\n",
        "test_tokenizer3.word_index\n",
        "\n",
        "test_sequence3 = test_tokenizer3.texts_to_sequences(test_texts3)\n",
        "test_sequence3\n",
        "\n",
        "tokenizer.fit_on_texts(texts) # texts are the whole datasets(train+test)\n",
        "sequences = tokenizer.texts_to_sequences(texts) #Converts a text to a sequence of words (or tokens).\n",
        "type(sequences)\n",
        "\n",
        "len(sequences)\n",
        "\n",
        "for i in range(5):\n",
        "  print(\"line %s review --- it is a %d ------\" %(i,labels[i]))\n",
        "  print(len(texts[i]),texts[i])\n",
        "  print(len(sequences[i]),sequences[i])\n",
        "\n",
        "for i in range(5):\n",
        "  print(\"line %s review --- it is a %d ------\" %(i,labels[i]))\n",
        "  print(len(texts[i].split()),texts[i])\n",
        "  print(\"tokenized sequence --------\")\n",
        "  print(len(sequences[i]),sequences[i])\n",
        "\n",
        "word_index = tokenizer.word_index #\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "type(word_index)\n",
        "word_index[\"this\"]\n",
        "\n",
        "word_index[\"i\"]\n",
        "word_index[\"the\"]\n",
        "\n",
        "\n",
        "\"\"\"  pad_sequences\n",
        "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "#x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "\n",
        "\"\"\"\n",
        "type(sequences)\n",
        "\n",
        "len_sequences_10 = [len(sequences[i]) for i in range(10)]\n",
        "len_sequences_10\n",
        "\n",
        "maxlen\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "len_data_10 = [len(data[i]) for i in range(10)]\n",
        "len_data_10\n",
        "\n",
        "labels = np.asarray(labels)\n",
        "print('Shape of data tensor:', data.shape)\n",
        "print('Shape of label tensor:', labels.shape)\n",
        "\n",
        "# Split the data into a training set and a validation set\n",
        "# But first, shuffle the data, since we started from data\n",
        "# where sample are ordered (all negative first, then all positive).\n",
        "indices = np.arange(data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "\n",
        "x_train = data[:training_samples]\n",
        "y_train = labels[:training_samples]\n",
        "x_val = data[training_samples: training_samples + validation_samples]\n",
        "y_val = labels[training_samples: training_samples + validation_samples]\n",
        "\n",
        "## Pre-process the embeddings\n",
        "Let's parse the un-zipped file (it's a txt file) to build an index mapping words (as strings) to their vector representation (as number vectors)\n",
        "\n",
        "glove_dir = 'resources/data/glove'\n",
        "\n",
        "embeddings_index = {}\n",
        "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "embeddings_index[\"love\"]\n",
        "\n",
        "len(embeddings_index[\"love\"])\n",
        "\n",
        "max_words #10000\n",
        "\n",
        "embedding_dim = 100\n",
        "\n",
        "# initiate the embedding_matrix with 0\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "embedding_matrix.shape #(10000, 100)\n",
        "\n",
        "#word_index.items() very long list from tokenizer\n",
        "\n",
        "\"\"\"\n",
        "some example:\n",
        "\n",
        "('sixteenth', 25417), \n",
        "('brethren', 25418), \n",
        "('pasty', 25419), \n",
        "('slinky', 25420), \n",
        "(\"office'\", 25421), \n",
        "('pd', 25422), \n",
        "\"\"\"\n",
        "max_words\n",
        "\n",
        "# get the embedding_matrix from pre-tainined embeddings\n",
        "for word, i in word_index.items(): #word_index is from the tokenizer\n",
        "    embedding_vector = embeddings_index.get(word) # get the embedding values( in this case is 100 dimension)\n",
        "    if i < max_words: # only use the most frequency word selected by max_words\n",
        "        if embedding_vector is not None:\n",
        "            # Words not found in embedding index will be all-zeros. from paddings\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "maxlen            \n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Flatten, Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.summary()\n",
        "\n",
        "model.layers[0].set_weights([embedding_matrix])\n",
        "model.layers[0].trainable = False\n",
        "\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])\n",
        "history_pre_train = model.fit(x_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=32,\n",
        "                    validation_data=(x_val, y_val))\n",
        "model.save_weights('tc_pre_trained_glove_model.h5')\n",
        "\n",
        "def plot_one(history):\n",
        "  \n",
        "  acc = history.history['acc']\n",
        "  val_acc = history.history['val_acc']\n",
        "  loss = history.history['loss']\n",
        "  val_loss = history.history['val_loss']\n",
        "\n",
        "  epochs = range(1, len(acc) + 1)\n",
        "\n",
        "  plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "  plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "  plt.title('Training and validation accuracy')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.figure()\n",
        "\n",
        "  plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "  plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "  plt.title('Training and validation loss')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.show()\n",
        "  \n",
        "# plot\n",
        "plot_one()\n",
        "\n",
        "## train the same model without loading the pre-trained word embeddings and without freezing the embedding layer. \n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=32,\n",
        "                    validation_data=(x_val, y_val))\n",
        "\n",
        "plot_one(history_pre_train)\n",
        "\n",
        "plot_one(history)\n",
        "\n",
        "test_dir = os.path.join(imdb_dir, 'test')\n",
        "\n",
        "labels = []\n",
        "texts = []\n",
        "\n",
        "for label_type in ['neg', 'pos']:\n",
        "    dir_name = os.path.join(test_dir, label_type)\n",
        "    for fname in sorted(os.listdir(dir_name)):\n",
        "        if fname[-4:] == '.txt':\n",
        "            f = open(os.path.join(dir_name, fname))\n",
        "            texts.append(f.read())\n",
        "            f.close()\n",
        "            if label_type == 'neg':\n",
        "                labels.append(0)\n",
        "            else:\n",
        "                labels.append(1)\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "x_test = pad_sequences(sequences, maxlen=maxlen)\n",
        "y_test = np.asarray(labels)\n",
        "\n",
        "\n",
        "model.evaluate(x_test, y_test)\n",
        "\n",
        "model.load_weights('pre_trained_glove_model.h5')\n",
        "model.evaluate(x_test, y_test)\n",
        "\n",
        "## pre trainined embedding is better\n",
        "max_features = 10000\n",
        "\n",
        "from keras.layers import LSTM\n",
        "\n",
        "model = Sequential()\n",
        "# Add an Embedding layer as before with 10000 vocab size(max features) \n",
        "#and 32 output dimensions\n",
        "# ...\n",
        "# Add a LSTM layer of 32 dimensions\n",
        "# ...\n",
        "# Exercise 3: Solution\n",
        "model.add(Embedding(max_features, 32))\n",
        "model.add(LSTM(32))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])\n",
        "history_lstm = model.fit(x_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=128,\n",
        "                    validation_split=0.2)\n",
        "plot_one(history_lstm)\n",
        "\n",
        "## using the imdb data to to train LSTM\n",
        "\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "\n",
        "max_features = 10000  # number of words to consider as features\n",
        "maxlen = 500  # cut texts after this number of words (among top max_features most common words)\n",
        "batch_size = 32\n",
        "\n",
        "print('Loading data...')\n",
        "(input_train, y_train), (input_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "print(len(input_train), 'train sequences')\n",
        "print(len(input_test), 'test sequences')\n",
        "\n",
        "print('Pad sequences (samples x time)')\n",
        "input_train = sequence.pad_sequences(input_train, maxlen=maxlen)\n",
        "input_test = sequence.pad_sequences(input_test, maxlen=maxlen)\n",
        "print('input_train shape:', input_train.shape)\n",
        "print('input_test shape:', input_test.shape)\n",
        "\n",
        "model = Sequential()\n",
        "# Add an Embedding layer as before with 10000 vocab size(max features) \n",
        "#and 32 output dimensions\n",
        "# ...\n",
        "# Add a LSTM layer of 32 dimensions\n",
        "# ...\n",
        "# Exercise 3: Solution\n",
        "model.add(Embedding(max_features, 32))\n",
        "model.add(LSTM(32))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])\n",
        "history_lstm = model.fit(input_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=128,\n",
        "                    validation_split=0.2)\n",
        "plot_one(history_lstm)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}